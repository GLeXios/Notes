

# 分布式

## 分布式架构知识体系

![pic_008](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16dd338cf97efdb7~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png)

## 场景分类

**<u>（1）分布式服务框架</u>**

​	主要分为HTTP和RPC两种类型，面向服务架构SOA，它是一种建设企业生态系统的架构指导思想。SOA的关注点是服务，服务最基本的业务功能是单元，由平台中立性的接口契约来定义。通过将业务系统服务化、不同模块解耦、各模块间互相调用、消息交换和资源共享。主流的分布式/微服务架构：**<font color = '#8D0101'>SpringBoot/Cloud、Dubbo、Tars、JSF、Motan</font>**等。

**<u>（2）分布式锁</u>**

- 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行。
- 高可用的获取锁与释放锁
- 高性能的获取锁与释放锁
- 具备可重入特性（可理解为重新进入，由多于一个任务并发使用，而不必担心数据错误）
- 具备锁失效机制，防止死锁
- 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败

**<font color = '#8D0101'>经典方案：基于zookeeper或者redis实现分布式锁。</font>**

**<u>（3）文件系统</u>**

​	单台计算机的存储始终有上限，随着网络的出现，多台计算机协作存储文件的方案也相继被提出来。最早的分布式文件系统其实也称为网络文件系统，第一个文件服务器在 1970 年代被发展出来。在 1976 年迪吉多公司设计出 File Access Listener（FAL），而现代分布式文件系统则出自赫赫有名的 Google 的论文，[《The Google File System》](https://link.juejin.cn?target=https%3A%2F%2Fstatic.googleusercontent.com%2Fmedia%2Fresearch.google.com%2Fzh-CN%2F%2Farchive%2Fgfs-sosp2003.pdf)奠定了分布式文件系统的基础。现代主流分布式文件系统参考[《分布式文件系统对比》](https://link.juejin.cn?target=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FComparison_of_distributed_file_systems),下面列举几个常用的文件系统：

- **<font color = '#8D0101'>HDFS、FastDFS、Ceph、mooseFS</font>**

**<u>（4）数据库</u>**

​	数据库当然也属于文件系统，主数据增加了事务、检索、擦除等高级特性，所以复杂度又增加了，既要考虑数据一致性也得保证足够的性能。传统关系型数据库为了兼顾事务和性能的特性，在分布式方面的发展有限，非关系型数据库摆脱了事务的强一致性束缚，达到了最终一致性的效果，从而有了飞跃的发展，**<font color = '#8D0101'>NoSql(Not Only Sql)</font>** 也产生了多个架构的数据库类型，包括 KV、列式存储、文档类型等。

- **<font color = '#8D0101'>列式存储：Hbase、文档存储：Elasticsearch，MongoDB、KV 类型：Redis、关系型：Spanner</font>**

**<u>（5）计算</u>**

​	分布式计算系统构建在分布式存储的基础上，充分发挥分布式系统的数据冗余灾备，多副本高效获取数据的特性，进而并行计算，把原本需要长时间计算的任务拆分成多个任务并行处理，从而提高了计算效率。分布式计算系统在场景上分为离线计算、实时计算和流式计算。

- **<font color = '#8D0101'>离线：Hadoop、实时：Spark、流式：Storm，Flink/Blink</font>**

**<u>（6）缓存</u>**

​	缓存作为提升性能的利器无处不在，小到 CPU 缓存架构，大到分布式应用存储。**<font color = '#8D0101'>分布式缓存系统</font>**提供了热点数据的随机访问机制，大大了提升了访问时间，但是带来的问题是如何保证数据的一致性，引入分布式锁来解决这个问题，主流的分布式存储系统基本就是 Redis 了。

- **<font color = '#8D0101'>持久化：Redis、非持久化：Memcache</font>**

**<u>（7）消息</u>**

​	**<font color = '#8D0101'>分布式消息队列系统</font>**是消除异步带来的一系列复杂步骤的一大利器，在多线程高并发场景下，我们常常需要谨慎设计业务代码，来保证多线程并发情况下不出现资源竞争导致的死锁问题。而消息队列以一种延迟消费的模式将异步任务都存到队列，然后再逐个消化。

- **<font color = '#8D0101'>Kafka、RabbitMQ、RocketMQ、ActiveMQ</font>**

**<u>（8）监控</u>**

​	分布式系统复杂，需要有一个监控来将整个应用调用的全过程进行全天候监控，也能对系统资源、第三方组件进行监控，确保能够第一时间发现问题并及时解决问题。

- web地址响应性能监控与统计
- 服务响应新能监控与统计
- RPC服务响应性能监控与统计
- API接口响应性能监控与统计
- 组件节点监控（MySQL、Redis、MQ）
- 系统CPU、内存、硬件监控
- 系统异常监控与统计

解决方案：**<font color = '#8D0101'>Zabbix、Nagios、Metrics、Spectator</font>**

<u>**（9）配置中心**</u>

​	随着业务的发展、微服务架构的升级，服务的数量、程序的配置日益增多（各种微服务、各种服务器地址、各种参数），传统的配置文件方式和数据库的方式已无法满足开发人员对配置管理的要求：

- 安全性：配置跟随源代码保存在代码库中，容易造成配置泄漏。
- 时效性：修改配置，需要重启服务才能生效。
- 局限性：无法支持动态调整：例如日志开关、功能开关。

**<font color = '#8D0101'>常见分布式配置中心：Spring config 、Apollo 、nacos、Diamond、Disconf</font>**  （详见）[SpringCloud-Config配置中心学习总结](https://mp.weixin.qq.com/s?spm=a2c6h.12873639.article-detail.9.d449698fEjbePZ&__biz=MzUzOTk5MzYyNg==&mid=2247483847&idx=1&sn=081fc6c3c6c8d1bfb22b643000c28247&chksm=fb3eb2b2cc493ba481c51ae5610cb1ceedb16560275bd07f5e17b8766d8c6cfa7dde34e1e7ea&token=2008267728&lang=zh_CN&scene=21#wechat_redirect)

<u>**（10）注册中心**</u>

注册中心主要涉及到三大角色：

1. 服务提供者
2. 服务消费者
3. 注册中心

关系大致如下：

- 各个微服务在启动时，将自己的网络地址等信息注册到注册中心，注册中心存储这些数据。
- 服务消费者从注册中心查询服务提供者的地址，并通过该地址调用服务提供者的接口。
- 各个微服务与注册中心使用一定机制（例如心跳）通信。如果注册中心与某微服务长时间无法通信，就会注销该实例。
- 微服务网络地址发送变化（例如实例增加或IP变动等）时，会重新注册到注册中心。这样，服务消费者就无需人工修改提供者的网络地址了。

**<font color = '#8D0101'>主要解决方案：Zookeeper、Eureka、Nacos、Consul</font>**

## 集群与分布式的区别

​	前段时间有同学和我探讨起分布式的东西，他说分布式不就是加机器吗？一台机器不够用再加一台抗压呗。当然加机器这种说法也无可厚非，你一个分布式系统必定涉及到多个机器，但是你别忘了，计算机学科中还有一个相似的概念—— `Cluster` ，集群不也是加机器吗？**但是 集群 和 分布式 其实就是两个完全不同的概念**。

- 比如，我现在有一个秒杀服务，并发量太大单机系统承受不住，那我加几台服务器也 **一样** 提供秒杀服务，这个时候就是 **`Cluster` 集群** 。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/60263e969b9e4a0f81724b1f4d5b3d58~tplv-k3u1fbpfcp-zoom-1.jpeg" alt="cluster" style="zoom:60%;" />

- 但是，我现在换一种方式，我将一个秒杀服务 **拆分成多个子服务** ，比如创建订单服务，增加积分服务，扣优惠券服务等等，**然后我将这些子服务都部署在不同的服务器上** ，这个时候就是 **`Distributed` 分布式** 。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/0d42e7b4249144b3a77a0c519216ae3d~tplv-k3u1fbpfcp-zoom-1.jpeg" alt="distributed" style="zoom:60%;" />

​	**加机器更加适用于构建集群，因为它真是<font color = '#8D0101'>只有加机器</font>**。而对于**分布式来说，你首先需要<font color = '#8D0101'>将业务进行拆分</font>，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。**

- 比如各个分布式组件如何协调起来，如何减少各个系统之间的耦合度，分布式事务的处理，如何去配置整个分布式系统等等。`ZooKeeper` 主要就是解决这些问题的。

## 分布式CAP & BASE理论详解

==**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**==

### CAP理论

**CAP** 也就是 <font color = '#8D0101'>**Consistency（一致性）**、**Availability（可用性）**、**Partition Tolerance（分区容错性）**</font> 这三个单词首字母组合。

CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：

- **一致性（Consistency）** : 所有节点访问同一份最新的数据副本
- **可用性（Availability）**: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。
- **分区容错性（Partition Tolerance）** : 分布式系统出现网络分区的时候，仍然能够对外提供服务。
  - **网络分区：**分布式系统中，多个节点之间的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫 **网络分区**

**<u>（1）不是所谓的“3 选 2”</u>**

​	**<font color = '#8D0101'>分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构</font>**。 比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。

> [!TIP]
>
> **当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是<font color = '#8D0101'>说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择</font>。也就是说分区容错性（Partition tolerance）我们是必须要实现的。**
>
> 简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。

- <u>**为啥不可能选择 CA 架构呢？**</u> eg：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。

- **选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。**

- 另外，需要补充说明的一点是：**如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。**

### BASE理论

​	<font color = '#8D0101'>**BASE** 是 **Basically Available（基本可用）**、**Soft-state（软状态）** 和 **Eventually Consistent（最终一致性）**</font> 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。

**<u>（1）核心思想</u>**

- **BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。**

- AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。


**<u>（2）基本可用</u>**

基本可用是指分布式系统在出现不可预知故障的时候，允许**损失部分可用性**。但是，这绝不等价于系统不可用。

**什么叫允许损失部分可用性呢？**

- **响应时间上的损失**: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。
- **系统功能上的损失**：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。

**<u>（3）软状态</u>**

​	软状态指允许系统中的数据存在中间状态（**CAP 理论中的数据不一致**），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。

**<u>（4）最终一致性</u>**

​	最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。

> 分布式一致性的 3 种级别：
>
> - **强一致性**：系统写入了什么，读出来的就是什么。
>
> - **弱一致性**：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。
>
> - **最终一致性**：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。
>
> **业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。**

### 总结

==**ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。**==

- 分区容错性在微服务中是躲不过的命题，可以这么说，只要是分布式，只要是集群都面临着AP或者CP的选择，但你很贪心的时候，既要一致性又要可用性，那只能对一致性作出一点妥协，也就是引入了BASE理论，在业务允许的情况下实现最终一致性。
- 究竟是选AP还是选CP，真的在于对业务的了解，**<font color = '#8D0101'>例如金钱，库存相关会优先考虑CP模型，例如社区发帖相关可以优先选择AP模型</font>**，这个说白了其实基于对业务的了解是一个选择和妥协的过程。

## 分布式事务解决方案

### 2PC（两阶段提交协议）

2PC 将事务的提交过程分为 2 个阶段：**准备阶段 和 提交阶段** 。

**【准备阶段（Prepare）】**

准备阶段的核心是“询问”事务参与者执行本地数据库事务操作是否成功。

**准备阶段的工作流程：**

1. 事务协调者/管理者（后文简称 TM）向所有涉及到的事务参与者（后文简称 RM）发送消息询问： “你是否可以执行事务操作呢？”，并等待其答复。
2. RM 接收到消息之后，开始执行本地数据库事务预操作比如写 redo log/undo log 日志，此时并不会提交事务。
3. RM 如果执行本地数据库事务操作成功，那就回复“Yes”表示我已就绪，否则就回复“No”表示我未就绪。

**【提交阶段（Commit）】**

提交阶段的核心是“询间“事务参与者提交本地事务是否成功。

**当所有事务参与者都是“就绪”状态的话：**

1. TM 向所有参与者发送消息：“你们可以提交事务啦！”（Commit消息）
2. RM 接收到 Commit 消息后执行提交本地数据库事务操作，执行完成之后释放整个事务期间所占用的资源。
3. RM 回复：“事务已经提交”（ACK 消息）。
4. TM 收到所有事务参与者的 ACK 消息之后，整个分布式事务过程正式结束。

**当任一事务参与者是“未就绪”状态的话：**

1. TM 向所有参与者发送消息：“你们可以执行回滚操作了！”（Rollback 消息）。

2. RM 接收到 Rollback 消息后执行本地数据库事务回滚执行完成之后释放整个事务期间所占用的资源。
3. RM 回复：“事务已经回滚”（ACK 消息）。
4. TM 收到所有RM 的 ACK消息之后，中断事务。

**【简单总结一下 2PC 两阶段中比较重要的一些点】**

1. 准备阶段的主要目的是测试 RM 能否执行本地数据库事务操作（！！！注意：这一步并不会提交事务）。
2. 提交阶段中 TM 会根据准备阶段中 RM 的消息来决定是执行事务提交还是回滚操作。
3. 提交阶段之后一定会结束当前的分布式事务

**【2PC 存在的问题】**

- **同步阻塞**：事务参与者会在正式提交事务之前会一直占用相关的资源。比如用户小明转账给小红，那其他事务也要操作用户小明或小红的话，就会阻塞。
- **数据不一致**：由于网络问题或者TM宕机都有可能会造成数据不一致的情况。比如在第2阶段（提交阶段），部分网络出现问题导致部分参与者收不到 Commit/Rollback 消息的话，就会导致数据不一致。
- **单点问题**：TM在其中也是一个很重要的角色，如果TM在准备（Prepare）阶段完成之后挂掉的话，事务参与者就会一直卡在提交（Commit）阶段。

### 消息队列MQ事务（kafka为例子）

​	Kafka 的事务机制基于 **两阶段提交（2PC）协议** ，通过协调生产者、消费者与 Kafka 集群之间的操作，确保跨分区和跨会话的原子性。

【**两阶段事务提交**】

**第一阶段：准备提交（Prepare Commit）**

- 生产者调用 commitTransaction API 后，协调者（Kafka集群中的一个Broker担任协调者，负责管理事务日志（`__transaction_state`主题））将事务状态更新为 “prepare_commit” ，并将该状态写入事务日志（持久化）。
- 协调者向所有涉及事务的分区（生产者写入的分区和消费者位移提交的分区）发送 事务控制消息 ，标记事务进入准备阶段 。

**第二阶段：正式提交（Commit）**

- 协调者等待所有分区确认准备完成后，向分区发送 提交指令 ，确保数据对消费者可见 。
- 若任一分区失败，协调者会触发回滚（abortTransaction），保证原子性 。

【**Flink两阶段提交协议的详细实现**】

​	两阶段提交协议包括预提交（Pre-Commit）和提交（Commit）两个阶段，具体在 Flink 和 Kafka 中的实现如下：

**预提交阶段**

- 当 Flink 发起 checkpoint 时，JobMaster（协调者）发送 start-checkpoint 信号。
- 各算子执行本地 checkpoint，包括 sink 算子（如 FlinkKafkaProducer）。在此阶段，sink 算子调用 beginTransaction 开始 Kafka 事务，并通过 preCommit 将数据刷新（flush）到 Kafka，但不提交。
- 同时，算子的状态（包括事务状态）被快照保存，确保故障恢复时能恢复一致性。

**提交阶段**

- 一旦所有算子完成本地 checkpoint 并返回确认，JobMaster 认为 checkpoint 成功，发送 notifyCheckpointComplete 回调。
- sink 算子收到回调后，调用 commit 方法，提交 Kafka 事务，使数据对消费者可见。





## CAP 实际应用案例

### 服务注册中心选择 AP

注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小

服务注册中心主要是解决什么问题：一个是服务注册，一个是服务发现。

- **服务注册**：实例将自身服务信息注册到注册中心，这部分信息包括服务的主机IP和服务的Port，以及暴露服务自身状态和访问协议信息等。

- **服务发现**：实例请求注册中心所依赖的服务信息，服务实例通过注册中心，获取到注册到其中的服务实例的信息，通过这些信息去请求它们提供的服务。

> [!TIP]
>
> 目前作为注册中心的一些组件大致有：**<font color = '#8D0101'>dubbo的zookeeper，springcloud的eureka，consul，rocketMq的nameServer，hdfs的nameNode</font>**。目前微服务主流是dubbo和springcloud，**使用最多是zookeeper和eureka**，我们就来看看应该根据CAP理论应该怎么去选择注册中心。（springcloud也可以用zk，不过不是主流不讨论）。

<u>**（1）zookeeper选择CP**</u>

​	zookeep保证CP，即**<font color = '#8D0101'>任何时刻对zookeeper的访问请求能得到一致性的数据结果</font>**，同时系统对网络分割具备容错性，但是它**<font color = '#8D0101'>不能保证每次服务的可用性</font>**。从实际情况来分析，在使用zookeeper获取服务列表时，如果zk正在选举或者zk集群中半数以上的机器不可用，那么将无法获取数据。所以说，zk不能保证服务可用性。

<u>**（2）eureka选择AP**</u>

​	eureka保证AP，eureka在设计时优先保证可用性，每一个节点都是平等的，一部分节点挂掉不会影响到正常节点的工作，不会出现类似zk的选举leader的过程，客户端发现向某个节点注册或连接失败，会自动切换到其他的节点，**<font color = '#8D0101'>只要有一台eureka存在，就可以保证整个服务处在可用状态，只不过有可能这个服务上的信息并不是最新的信息。</font>**

**<u>（3）zookeeper和eureka的数据一致性问题</u>**

<u>**1）zookeeper**</u>	

​	先要明确一点，eureka的创建初心就是为一个注册中心，但是zk更多是作为分布式协调服务的存在，只不过因为它的特性被dubbo赋予了注册中心，z**<font color = '#8D0101'>k的职责更多是保证数据（配置数据，状态数据）在管辖下的所有服务之间保持一致</font>**，所有这个就不难理解为何zk被设计成CP而不是AP，zk最核心的算法ZAB，就是为了解决分布式系统下数据在多个服务之间一致同步的问题。

​	更深层的原因，zookeeper是按照CP原则构建，也就是说它必须保持每一个节点的数据都保持一致，如果zookeeper下节点断开或者集群中出现网络分割（例如交换机的子网间不能互访），那么zk会将它们从自己的管理范围中剔除，外界不能访问这些节点，即使这些节点是健康的可以提供正常的服务，所以导致这些节点请求都会丢失。

**<u>2）eureka</u>**

- eureka则完全没有这方面的顾虑，它的**<font color = '#8D0101'>节点都是相对独立，不需要考虑数据一致性的问题</font>**，这个应该是eureka的诞生就是为了注册中心而设计，相对zk来说剔除了leader节点选取和事务日志极致，这样更有利于维护和保证eureka在运行的健壮性。
- 数据不一致性在注册服务中中会给eureka带来什么问题：
  - 无非就是某一个节点被注册的服务多，某个节点注册的服务少，在某一个瞬间可能导致某些ip节点被调用数少，某些ip节点调用数少的问题。也有可能存在一些本应该被删除而没被删除的脏数据。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d05888fc2f78b3~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom:67%;" />

<u>**（3）小结：服务注册应该选择AP**</u>

​	对于服务注册来说，针对同一个服务，即使注册中心的不同节点保存的服务注册信息不相同，也并不会造成灾难性的后果，对于服务消费者来说，能消费才是最重要的，**就算拿到的数据不是最新的数据，消费者本身也可以进行尝试失败重试**。总比为了追求数据的一致性而获取不到实例信息整个服务不可用要好。所以，对于服务注册来说，**<font color = '#8D0101'>可用性比数据一致性更加的重要，选择AP。</font>**

### 分布式锁选择AP or CP ？

这里实现分布式锁的方式选取了三种：

- 基于数据库实现分布式锁
- 基于redis实现分布式锁
- 基于zookeeper实现分布式锁

#### 基于数据库实现分布式锁

构建表结构

![image](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d0588a12368882~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png)

利用表的 UNIQUE KEY `idx_lock` (`method_lock`) 作为唯一主键，当进行上锁时进行insert动作，数据库成功录入则以为上锁成功，当数据库报出 Duplicate entry 则表示无法获取该锁。

![image](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d0588a11076d99~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png)

不过这种方式对于单主却无法自动切换主从的mysql来说，基本就无法现实P分区容错性，（Mysql自动主从切换在目前并没有十分完美的解决方案）。可以说这种方式强依赖于数据库的可用性，数据库写操作是一个单点，一旦数据库挂掉，就导致锁的不可用。这种方式基本不在CAP的一个讨论范围。

#### 基于redis实现分布式锁-AP

edis单线程串行处理天然就是解决串行化问题，用来解决分布式锁是再适合不过。

实现方式：

```
setnx key value Expire_time
获取到锁 返回 1 ， 获取失败 返回 0
```

**<u>（1）redis实现分布式锁的过程</u>**

为了解决数据库锁的无主从切换的问题，可以选择redis集群，或者是 sentinel 哨兵模式，实现主从故障转移，当master节点出现故障，哨兵会从slave中选取节点，重新变成新的master节点。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d05889f092fb7f~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom: 55%;" />

- 哨兵模式故障转移是由sentinel集群进行监控判断，当master出现异常即复制中止，重新推选新slave成为master，sentinel在重新进行选举并不在意主从数据是否复制完毕具备一致性。
- **<font color = '#8D0101'>所以redis的复制模式是属于AP的模式</font>**。保证可用性，在主从复制中“主”有数据，但是可能“从”还没有数据，这个时候，一旦主挂掉或者网络抖动等各种原因，可能会切换到“从”节点，这个时候可能会导致两个业务线程同时获取得两把锁

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d05889f24be477~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom:80%;" />

- 上述的问题其实并不是redis的缺陷，只是redis采用了AP模型，它本身无法确保我们对一致性的要求。redis官方推荐redlock算法来保证，问题是redlock至少需要三个redis主从实例来实现，维护成本比较高，相当于redlock使用三个redis集群实现了自己的另一套一致性算法，比较繁琐，在业界也使用得比较少。

**<u>（2）是否使用redis作为分布式锁</u>**

能不能使用redis作为分布式锁，这个本身就不是redis的问题，还是取决于业务场景，我们先要自己确认我们的场景是适合 AP 还是 CP ，

- 如果在社交发帖等场景下，我们并没有非常强的事务一致性问题，redis提供给我们高性能的AP模型是非常适合的，

- 但如果是交易类型，对数据一致性非常敏感的场景，我们可能要寻在一种更加适合的 CP 模型

#### 基于zookeeper实现分布式锁-CP

zk的模式是CP模型，也就是说，当**<font color = '#8D0101'>zk锁提供给我们进行访问的时候，在zk集群中能确保这把锁在zk的每一个节点都存在</font>**。

> [!TIP]
>
> 这个实际上是zk的leader通过二阶段提交写请求来保证的，这个也是zk的集群规模大了的一个瓶颈点

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d05889f6a5ce1c~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom:80%;" />

**<u>（1）zk锁实现的原理</u>**

**<u>1）zookeeper的几个特性</u>**

**有序节点**

- 当在一个父目录下如 /lock 下创建有序节点，节点会按照严格的先后顺序创建出自节点 lock000001,lock000002,lock0000003,以此类推，有序节点能严格保证各个自节点按照排序命名生成。

**临时节点**

- 客户端建立了一个临时节点，在客户端的会话结束或会话超时，zookepper会自动删除该节点。

**事件监听**

- 在读取数据时，我们可以对节点设置监听，当节点的数据发生变化（1 节点创建、2 节点删除）时，zookeeper会通知客户端。

**<u>2）zk如何组合分布式锁</u>**

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d0588a51620561~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom:80%;" />

1. 业务线程-1 业务线程-2 分别向zk的/lock目录下，申请创建有序的临时节点
2. 业务线程-1 抢到/lock0001 的文件，也就是在整个目录下最小序的节点，也就是线程-1获取到了锁
3. 业务线程-2 只能抢到/lock0002的文件，并不是最小序的节点，线程2未能获取锁
4. 业务线程-1 与 lock0001 建立了连接，并维持了心跳，维持的心跳也就是这把锁的租期
5. 当业务线程-1 完成了业务，将释放掉与zk的连接，也就是释放了这把锁

**<u>3）zk分布式锁的代码实现</u>**

zk官方提供的客户端并不支持分布式锁的直接实现，我们需要自己写代码去利用zk的这几个特性去进行实现。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16d0588a68844a5a~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.png" alt="image" style="zoom:80%;" />

#### 小结：究竟该用CP还是AP的分布式锁

首先得了解清楚我们使用分布式锁的场景，为何使用分布式锁，用它来帮我们解决什么问题，先聊场景后聊分布式锁的技术选型。

- redis的AP模型会限制很多使用场景，但它却拥有了几者中最高的性能，

- zookeeper的分布式锁要比redis可靠很多，但他繁琐的实现机制导致了它的性能不如redis，而且zk会随着集群的扩大而性能更加下降。

简单来说，先了解业务场景，后进行技术选型。

## Gossip 协议详解

在分布式系统中，不同的节点进行数据/信息共享是一个基本的需求。

一种比较简单粗暴的方法就是 **集中式发散消息**，简单来说就是一个主节点同时共享最新信息给其他所有节点，比较适合中心化系统。这种方法的缺陷也很明显，节点多的时候不光同步消息的效率低，还太依赖与中心节点，存在单点风险问题。

于是，<font color = '#8D0101'>**分散式发散消息** 的 **Gossip 协议**</font> 就诞生了。

**<u>（1）Gossip 协议介绍</u>**

**Gossip 协议** 也叫 Epidemic 协议（流行病协议）或者 Epidemic propagation 算法（疫情传播算法）

- 正如 Gossip 协议其名一样，这是一种随机且带有传染性的方式将信息传播到整个网络中，并在一定时间内，使得系统内的所有节点数据一致。
- 在 Gossip 协议下，没有所谓的中心节点，每个节点周期性地随机找一个节点互相同步彼此的信息，理论上来说，各个节点的状态最终会保持一致

<font color = '#8D0101'>**Gossip 协议是一种允许在分布式系统中共享状态的去中心化通信协议，通过这种通信协议，我们可以将信息传播给网络或集群中的所有成员**</font>

**<u>（2）Gossip 协议应用</u>**

​	我们这里以 Redis Cluster 为例说明 Gossip 协议的实际应用。**分布式缓存** Redis 的官方集群解决方案（3.0 版本引入）， **<font color = '#8D0101'>Redis Cluster 就是基于 Gossip 协议来实现集群中各个节点数据的最终一致性</font>**。

​	Redis Cluster 是一个典型的分布式系统，分布式系统中的各个节点需要互相通信。既然要相互通信就要遵循一致的通信协议，Redis Cluster 中的各个节点基于 **Gossip 协议** 来进行通信共享信息，**<font color = '#8D0101'>每个 Redis 节点都维护了一份集群的状态信息。</font>**

Redis Cluster 的节点之间会相互发送多种 Gossip 消息：

- **MEET**：在 Redis Cluster 中的某个 Redis 节点上执行 `CLUSTER MEET ip port` 命令，可以向指定的 Redis 节点发送一条 MEET 信息，用于将其添加进 Redis Cluster 成为新的 Redis 节点。
- **PING/PONG**：Redis Cluster 中的节点都会定时地向其他节点发送 PING 消息，来交换各个节点状态信息，检查各个节点状态，包括在线状态、疑似下线状态 PFAIL 和已下线状态 FAIL。
- **FAIL**：Redis Cluster 中的节点 A 发现 B 节点 PFAIL ，并且在下线报告的有效期限内集群中半数以上的节点将 B 节点标记为 PFAIL，节点 A 就会向集群广播一条 FAIL 消息，通知其他节点将故障节点 B 标记为 FAIL 。
- .......

下图就是主从架构的 Redis Cluster 的示意图，图中的虚线代表的就是各个节点之间使用 Gossip 进行通信 ，实线表示主从复制。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/redis-cluster-gossip-B87ioOq3.png" alt="img" style="zoom:50%;" />

有了 Redis Cluster 之后，不需要专门部署 Sentinel 集群服务了。Redis Cluster 相当于是内置了 Sentinel 机制，Redis Cluster 内部的各个 Redis 节点通过 Gossip 协议共享集群内信息。

**<u>（3）Gossip 协议优势和缺陷</u>**

**优势：**

1、相比于其他分布式协议/算法来说，Gossip 协议理解起来非常简单。

2、能够容忍网络上节点的随意地增加或者减少，宕机或者重启，因为 Gossip 协议下这些节点都是平等的，去中心化的。新增加或者重启的节点在理想情况下最终是一定会和其他节点的状态达到一致。

3、速度相对较快。节点数量比较多的情况下，扩散速度比一个主节点向其他节点传播信息要更快（多播）。

**-**

**缺陷** :

1、消息需要通过多个传播的轮次才能传播到整个网络中，因此，必然会出现各节点状态不一致的情况。毕竟，Gossip 协议强调的是最终一致，至于达到各个节点的状态一致需要多长时间，谁也无从得知。

2、由于拜占庭将军问题，不允许存在恶意节点。

3、可能会出现消息冗余的问题。由于消息传播的随机性，同一个节点可能会重复收到相同的消息。

**<u>（4）总结</u>**

- Gossip 协议是一种允许在分布式系统中共享状态的通信协议，通过这种通信协议，我们可以将信息传播给网络或集群中的所有成员。
- Gossip 协议被 Redis、Apache Cassandra、Consul 等项目应用。
- 谣言传播（Rumor-Mongering）比较适合节点数量比较多或者节点动态变化的场景。

## Api网关

### Api网关基础知识

<u>**（1）api网关简介**</u>

​	微服务背景下，一个系统被拆分为多个服务，但是像安全认证，流量控制，日志，监控等功能是每个服务都需要的，没有网关的话，我们就需要在每个服务中单独实现，这使得我们做了很多重复的事情并且没有一个全局的视图来统一管理这些功能。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/api-gateway-overview.png" alt="网关示意图" style="zoom:50%;" />

​	一般情况下，网关可以为我们提供请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、降级熔断、日志、监控、参数校验、协议转换等功能。

- 实际上，网关主要做了两件事情：<font color = '#8D0101'>**请求转发** + **请求过滤**</font>。
- 我们需要保障网关服务的高可用，避免单点风险。

- **网关服务外层通过 Nginx（其他负载均衡设备/软件也行） 进⾏负载转发以达到⾼可⽤**。Nginx 在部署的时候，尽量也要考虑高可用，避免单点风险。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/server-load-balancing.png" alt="基于 Nginx 的服务端负载均衡" style="zoom:67%;" />

**<u>（2）网关能提供哪些功能？</u>**

绝大部分网关可以提供下面这些功能（有一些功能需要借助其他框架或者中间件）：

- **请求转发**：将请求转发到目标微服务。
- **负载均衡**：根据各个微服务实例的负载情况或者具体的负载均衡策略配置对请求实现动态的负载均衡。
- **安全认证**：对用户请求进行身份验证并仅允许可信客户端访问 API，并且还能够使用类似 RBAC 等方式来授权。
- **参数校验**：支持参数映射与校验逻辑。
- **日志记录**：记录所有请求的行为日志供后续使用。
- **监控告警**：从业务指标、机器指标、JVM 指标等方面进行监控并提供配套的告警机制。

- 流量控制：对请求的流量进行控制，也就是限制某一时刻内的请求数。
- **熔断降级**：实时监控请求的统计信息，达到配置的失败阈值后，自动熔断，返回默认值。
- **响应缓存**：当用户请求获取的是一些静态的或更新不频繁的数据时，一段时间内多次请求获取到的数据很可能是一样的。对于这种情况可以将响应缓存起来。这样用户请求可以直接在网关层得到响应数据，无需再去访问业务服务，减轻业务服务的负担。
- **响应聚合**：某些情况下用户请求要获取的响应内容可能会来自于多个业务服务。网关作为业务服务的调用方，可以把多个服务的响应整合起来，再一并返回给用户。
- **灰度发布**：将请求动态分流到不同的服务版本（最基本的一种灰度发布）。
- **异常处理**：对于业务服务返回的异常响应，可以在网关层在返回给用户之前做转换处理。这样可以把一些业务侧返回的异常细节隐藏，转换成用户友好的错误提示返回。
- **API 文档：** 如果计划将 API 暴露给组织以外的开发人员，那么必须考虑使用 API 文档，例如 Swagger 或 OpenAPI。
- **协议转换**：通过协议转换整合后台基于 REST、AMQP、Dubbo 等不同风格和实现技术的微服务，面向 Web Mobile、开放平台等特定客户端提供统一服务。
- **证书管理**：将 SSL 证书部署到 API 网关，由一个统一的入口管理接口，降低了证书更换时的复杂度。

**<u>（3）有哪些常见的网关系统？</u>**

**<u>1）SpringCloud Gateway</u>** 

​	SpringCloud Gateway 属于 Spring Cloud 生态系统中的网关，其诞生的目标是为了替代老牌网关 **Zuul**。准确点来说，应该是 Zuul 1.x。SpringCloud Gateway 起步要比 Zuul 2.x 更早。

​	为了提升网关的性能，SpringCloud Gateway 基于 **Spring WebFlux** 。Spring WebFlux 使用 **Reactor 库**来实现响应式编程模型，**底层基于 Netty 实现同步非阻塞的 I/O**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/springcloud-gateway-%20demo.png" alt="img" style="zoom:50%;" />

​	Spring Cloud Gateway 不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，限流。

​	Spring Cloud Gateway 和 Zuul 2.x 的差别不大，也是通过过滤器来处理请求。不过，目前更加推荐使用 Spring Cloud Gateway 而非 Zuul，Spring Cloud 生态对其支持更加友好。

**<u>2）OpenResty</u>**

根据官方介绍：

> OpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。

OpenResty 基于 Nginx，主要还是看中了其优秀的高并发能力。不过，由于 Nginx 采用 C 语言开发，二次开发门槛较高。如果想在 Nginx 上实现一些自定义的逻辑或功能，就需要编写 C 语言的模块，并重新编译 Nginx。

为了解决这个问题，OpenResty 通过实现 `ngx_lua` 和 `stream_lua` 等 Nginx 模块，把 Lua/LuaJIT 完美地整合进了 Nginx，从而让我们能够在 Nginx 内部里嵌入 Lua 脚本，使得可以通过简单的 Lua 语言来扩展网关的功能，比如实现自定义的路由规则、过滤器、缓存策略等。

> Lua 是一种非常快速的动态脚本语言，它的运行速度接近于 C 语言。LuaJIT 是 Lua 的一个即时编译器，它可以显著提高 Lua 代码的执行效率。LuaJIT 将一些常用的 Lua 函数和工具库预编译并缓存，这样在下次调用时就可以直接使用缓存的字节码，从而大大加快了执行速度。

### SpringCloudGateway

#### Spring Cloud Gateway 的工作流程

Spring Cloud Gateway 的工作流程如下图所示：

![image-20251018233308873](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/image-20251018233308873.png)

具体的流程分析：

1. **路由判断**：客户端的请求到达网关后，先经过 Gateway Handler Mapping 处理，这里面会做**断言（Predicate）判断，看下符合哪个路由规则，这个路由映射后端的某个服务**。
2. **请求过滤**：然后请求到达 Gateway Web Handler，这里面有很多过滤器，组成过滤器链（Filter Chain），这些**过滤器可以对请求进行拦截和修改，比如添加请求头、参数校验等等**，有点像净化污水。然后将请求转发到实际的后端服务。这些过滤器逻辑上可以称作 **Pre-Filters**，Pre 可以理解为“在...之前”。
3. **服务处理**：后端服务会对请求进行处理。
4. **响应过滤**：后端处理完结果后，返回给 Gateway 的过滤器再次做处理，逻辑上可以称作 **Post-Filters**，Post 可以理解为“在...之后”。
5. **响应返回**：响应经过过滤处理后，返回给客户端。

​	总结：客户端的请求先通过匹配规则找到合适的路由，就能映射到具体的服务。然后请求经过过滤器处理后转发给具体的服务，服务处理后，再次经过过滤器处理，最后返回给客户端。

#### Spring Cloud Gateway 的断言和路由

**<u>（1）断言</u>**

​	断言（Predicate）是一种编程术语。

​	在 Gateway 中，如果客户端发送的请求满足了断言的条件，则映射到指定的路由器，就能转发到指定的服务上进行处理。

​	断言配置的示例如下：配置了两个路由规则，有一个 predicates 断言配置，当请求 url 中包含 `api/thirdparty`，就匹配到了第一个路由 `route_thirdparty`。

![image-20251018233332064](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/image-20251018233332064.png)

**<u>（2）断言和路由的关系</u>**

Route 路由和 Predicate 断言的对应关系如下：

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/spring-cloud-gateway-predicate-route.png" alt="路由和断言的对应关系" style="zoom:45%;" />

- **一对多**：一个路由规则可以包含多个断言。如上图中路由 Route1 配置了三个断言 Predicate。
- **同时满足**：如果一个路由规则中有多个断言，则需要同时满足才能匹配。如上图中路由 Route2 配置了两个断言，客户端发送的请求必须同时满足这两个断言，才能匹配路由 Route2。
- **第一个匹配成功**：如果一个请求可以匹配多个路由，则映射第一个匹配成功的路由。如上图所示，客户端发送的请求满足 Route3 和 Route4 的断言，但是 Route3 的配置在配置文件中靠前，所以只会匹配 Route3。

#### Spring Cloud Gateway 如何实现动态路由

- Spring Cloud Gateway 作为微服务的入口，需要尽量避免重启，而**现在配置更改需要重启服务**不能满足实际生产过程中的动态刷新、实时变更的业务需求，所以我们**需要在 Spring Cloud Gateway 运行时动态配置网关**。

- 实现动态路由的方式有很多种，其中一种推荐的方式是**基于 Nacos 注册中心**来做。 Spring Cloud Gateway 可以从注册中心获取服务的元数据（例如服务名称、路径等），然后根据这些信息**自动生成路由规则**。这样，当你添加、移除或更新服务实例时，网关会自动感知并相应地调整路由规则，无需手动维护路由配置。

- 通过 Nacos Server 和 Spring Cloud Alibaba Nacos Config 即可实现配置的动态变更

#### Spring Cloud Gateway 的过滤器

过滤器 Filter 按照请求和响应可以分为两种：

- **Pre 类型**：在请求被转发到微服务之前，对请求进行拦截和修改，例如参数校验、权限校验、流量监控、日志输出以及协议转换等操作。
- **Post 类型**：微服务处理完请求后，返回响应给网关，网关可以再次进行处理，例如修改响应内容或响应头、日志输出、流量监控等。

另外一种分类是按照过滤器 Filter 作用的范围进行划分：

- **GatewayFilter**：局部过滤器，应用在单个路由或一组路由上的过滤器。标红色表示比较常用的过滤器。
- **GlobalFilter**：全局过滤器，应用在所有路由上的过滤器

**<u>（1）局部过滤器</u>**

具体怎么用呢？这里有个示例，如果 URL 匹配成功，则去掉 URL 中的 “api”。

```yaml
filters: #过滤器
  - RewritePath=/api/(?<segment>.*),/$\{segment} # 将跳转路径中包含的 “api” 替换成空
```

当然我们也可以自定义过滤器，本篇不做展开。

**<u>（2）全局过滤器</u>**

全局过滤器最常见的用法是进行负载均衡。配置如下所示：

```yaml
spring:
  cloud:
    gateway:
      routes:
        - id: route_member # 第三方微服务路由规则
          uri: lb://passjava-member # 负载均衡，将请求转发到注册中心注册的 passjava-member 服务
          predicates: # 断言
            - Path=/api/member/** # 如果前端请求路径包含 api/member，则应用这条路由规则
          filters: #过滤器
            - RewritePath=/api/(?<segment>.*),/$\{segment} # 将跳转路径中包含的api替换成空
```

​	这里有个关键字 `lb`，用到了全局过滤器 `LoadBalancerClientFilter`，当匹配到这个路由后，会将请求转发到 passjava-member 服务，且支持负载均衡转发，也就是先将 passjava-member 解析成实际的微服务的 host 和 port，然后再转发给实际的微服务。

#### Spring Cloud Gateway 自定义全局异常处理

​	在 SpringBoot 项目中，我们捕获全局异常只需要在项目中配置 **`@RestControllerAdvice`和 `@ExceptionHandler`**就可以了。不过，这种方式在 Spring Cloud Gateway 下不适用。

​	Spring Cloud Gateway 提供了多种全局处理的方式，比较常用的一种是实现**`ErrorWebExceptionHandler`并重写其中的`handle`方法。**

```java
@Order(-1)
@Component
@RequiredArgsConstructor
public class GlobalErrorWebExceptionHandler implements ErrorWebExceptionHandler {
    private final ObjectMapper objectMapper;

    @Override
    public Mono<Void> handle(ServerWebExchange exchange, Throwable ex) {
    // ...
    }
}
```

## RPC

### RPC简介与过程

​	**RPC（Remote Procedure Call）** 即远程过程调用，RPC 关注的是远程调用而非本地调用，是分布式系统常见的一种通信方法，**允许程序调用另一个地址空间（通常是共享网络的另一台机器上）的过程或函数**，而不用程序员显式编码这个远程调用的细节

**简单的说：**

- RPC就是从一台机器（客户端）上**通过参数传递的方式调用**另一台机器（服务器）上的一个函数或方法（可以统称为服务）并得到返回的结果。
- RPC会**隐藏底层的通讯细节**（不需要直接处理Socket通讯或Http通讯）。
- 客户端发起请求，服务器返回响应（类似于Http的工作方式）RPC在**使用形式上像调用本地函数（或方法）一样去调用远程的函数（或方法）**。

**<font color = '#8D0101'>RPC 是一种技术思想、一种调用方式而非一种规范或协议如HTTP</font>**，常见 RPC 技术和框架有：

- 应用级的服务框架：阿里的 Dubbo/Dubbox、Google gRPC、Spring Boot/Spring Cloud。
- 远程通信协议：RMI、Socket、SOAP(HTTP XML)、REST(HTTP JSON)。
- 通信框架：MINA 和 Netty。

**<u>（1）一个完整的RPC框架</u>**

​	在一个典型 RPC 的使用场景中，包含了服务发现、负载、容错、网络传输、序列化等组件，其中“RPC 协议”就指明了程序**如何进行网络传输和序列化。**

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/9117ca82c507452db11b380e45ef9a94.jpeg" alt="img" style="zoom:45%;" />

<u>**（2）RPC过程**</u>

具体原理图如下：

![RPC原理图](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/37345851.jpg)

- 服务消费端（client）以本地调用的方式调用远程服务；
- 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（**序列化**）：`RpcRequest`；
- 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端；
- 服务端 Stub（桩）收到消息将消息**反序列化为 Java 对象**: `RpcRequest`；
- 服务端 Stub（桩）根据`RpcRequest`中的类、方法、方法参数等信息调用本地的方法；
- 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：`RpcResponse`（序列化）发送至消费方；
- 客户端 Stub（client stub）接收到消息并将消息**反序列化为 Java 对象**:`RpcResponse` ，这样也就得到了最终结果

### RPC原理

**为此rpc需要解决三个问题（实现的关键）：**

![img](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/458e6ff3698c49619566dae9a77f2005.png)

<u>**1）服务寻址。**</u>我们怎么告诉远程机器（**注册中心**）我们要调用哪个函数呢？

- 服务寻址可以使用 **Call ID 映射**，在本地调用中，函数体是直接通过函数指针来指定的，我们调用具体函数，编译器就自动帮我们调用它相应的函数指针。
- 但是在远程调用中，是无法调用函数指针的，因为两个进程的地址空间是完全不一样。所以，在RPC中，**所有的函数都必须有自己的一个ID**。这个ID在所有进程中都是唯一确定的。客户端在做远程过程调用时，必须附上这个ID。然后我们还**需要在客户端和服务端分别维护一个 {函数 <--> Call ID} 的对应表**。两者的表不一定需要完全相同，但相同的函数对应的Call ID必须相同。当客户端需要进行远程调用时，它就查一下这个表，找出相应的Call ID，然后把它传给服务端，服务端也通过查表，来确定客户端需要调用的函数，然后执行相应函数的代码。
- **<font color = '#8D0101'>实现方式：服务注册中心</font>**
  - 要调用服务，首先你需要一个服务注册中心去查询对方服务都有哪些实例。Dubbo 的服务注册中心是可以配置的，官方推荐使用 Zookeeper。

<u>**2）序列化和反序列化**</u>。客户端**怎么把参数值传给远程的函数呢？**

- 在本地调用中，我们只需要把参数压到栈里，然后让函数自己去栈里读就行。
- 但是**在远程过程调用时，客户端跟服务端是不同的进程，不能通过内存来传递参数**。甚至有时候客户端和服务端使用的都不是同一种语言（比如服务端用C++，客户端用Java或者Python）。这时候就**需要客户端把参数先转成一个字节流，传给服务端后，再把字节流转成自己能读取的格式。这个过程叫序列化和反序列化（将二进制流转换成对象的过程叫做反序列化）**。同理，从服务端返回的值也需要序列化反序列化的过程。

**<u>3）数据网络传输</u>。**

- 远程调用往往是基于网络的，客户端和服务端是通过网络连接的。**所有的数据都需要通过网络传输，因此就需要有一个网络传输层。**

- **网络传输层需要把Call ID和序列化后的参数字节流传给服务端，然后再把序列化后的调用结果传回客户端**。只要能完成这两者的，都可以作为传输层使用。因此，它所使用的协议其实是不限的，能完成传输就行。

  - 大部分RPC框架都使用TCP协议：
    - TCP 的连接是最常见的，简要分析基于 TCP 的连接：通常 TCP 连接可以是按需连接（需要调用的时候就先建立连接，调用结束后就立马断掉），也可以是长连接（客户端和服务器建立起连接之后保持长期持有，不管此时有无数据包的发送，可以配合心跳检测机制定期检测建立的连接是否存活有效），多个远程过程调用共享同一个连接。

  - 其实UDP也可以，而gRPC干脆就用了HTTP2。Java的Netty（**基于NIO通信方式作为高性能网络服务的前提**）也属于这层的东西。

**<u>1️⃣基于 TCP 协议的 RPC 调用</u>**

- 由服务的调用方与服务的提供方建立 Socket 连接，并由服务的调用方通过 Socket 将需要调用的接口名称、方法名称和参数序列化后传递给服务的提供方，服务的提供方反序列化后再利用反射调用相关的方法。最后将结果返回给服务的调用方，整个基于 TCP 协议的 RPC 调用大致如此
- 但是在实例应用中则会进行一系列的封装，如 RMI 便是在 TCP 协议上传递可序列化的 Java 对象。

<u>**2️⃣基于 HTTP 协议的 RPC 调用**</u>

该方法更像是访问网页一样，只是它的返回结果更加单一简单。

- 其大致流程为：由服务的调用者向服务的提供者发送请求，这种请求的方式可能是 GET、POST、PUT、DELETE 等中的一种，服务的提供者可能会根据不同的请求方式做出不同的处理，或者某个方法只允许某种请求方式。

- 而调用的具体方法则是根据 URL 进行方法调用，而方法所需要的参数可能是对服务调用方传输过去的 XML 数据或者 JSON 数据解析后的结果，最后返回 JOSN 或者 XML 的数据结果。

由于目前有很多开源的 Web 服务器，如 Tomcat，所以其实现起来更加容易，就像做 Web 项目一样。

**<u>3️⃣基于TCP和HTTP的对比</u>**

- 基于 TCP 的协议实现的 RPC 调用
  - 由于 TCP 协议处于协议栈的下层，能够更加灵活地对协议字段进行定制，减少网络开销，提高性能，实现更大的吞吐量和并发数。
  - 但是需要更多关注底层复杂的细节，实现的代价更高。同时对不同平台，如安卓，iOS 等，需要重新开发出不同的工具包来进行请求发送和相应解析，工作量大，难以快速响应和满足用户需求。

- 基于 HTTP 协议实现的 RPC 则可以使用 JSON 和 XML 格式的请求或响应数据
  - 而 JSON 和 XML 作为通用的格式标准（使用 HTTP 协议也需要序列化和反序列化，不过这不是该协议下关心的内容，成熟的 Web 程序已经做好了序列化内容），开源的解析工具已经相当成熟，在其上进行二次开发会非常便捷和简单。
  - 但是由于 HTTP 协议是上层协议，发送包含同等内容的信息，使用 HTTP 协议传输所占用的字节数会比使用 TCP 协议传输所占用的字节数更高。
  - 因此在同等网络下，通过 HTTP 协议传输相同内容，效率会比基于 TCP 协议的数据效率要低，信息传输所占用的时间也会更长，当然压缩数据，能够缩小这一差距。



**<u>4）设计一个高性能、高可靠、高可用的RPC框架（需要考虑的问题）</u>**

简单实现一个 RPC 框架，只需要把以下三点实现了就基本完成了：

- Call ID 映射：可以直接使用函数字符串，也可以使用整数 ID。映射表一般就是一个哈希表。
- 序列化反序列化：可以自己写，也可以使用 Protobuf 或者 FlatBuffers 之类的。
- 网络传输库：可以自己写 Socket，或者用 Asio，ZeroMQ，Netty 之类。

**设计一个高性能的RPC框架**

- **如何解决获取实例的问题**？
  - 既然系统采用分布式架构，那一个服务势必会有多个实例，所以需要**一个服务注册中心**，比如在Dubbo中，就可以使用Zookeeper作为注册中心，在调用时，从**Zookeeper**获取服务的实例列表，再从中选择一个进行调用。也可以同**Nacos**做服务注册中心；

- **如何选择实例？**
  - 就要考虑**负载均衡**，例如dubbo提供了4种负载均衡策略；

- 如果每次都去注册中心查询列表，效率很低，那么就要加**缓存**；

- 客户端总不能每次调用完都等着服务端返回数据，所以就要支持**异步调用**；

- 服务端的接口修改了，老的接口还有人在用，这就需要**版本控制**；

- 服务端总不能每次接到请求都马上启动一个线程去处理，于是就需要**线程池**；

### 有了 HTTP 协议，为什么还要有 RPC

**<u>（1）从TCP聊起</u>**

​	假设我们需要在 A 电脑的进程发一段数据到 B 电脑的进程，我们一般会在代码里使用 **`socket`** 进行编程。这时候，我们可选项一般也就**TCP 和 UDP 二选一。TCP 可靠，UDP 不可靠。**类似下面这样。

```ini
fd = socket(AF_INET,SOCK_STREAM,0);
```

其中`SOCK_STREAM`，是指使用**字节流**传输数据，说白了就是**TCP 协议**。在定义了 socket 之后，我们就可以对这个 socket 进行操作，比如用`bind()`绑定 IP 端口，用`connect()`发起连接。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/f410977cda814d32b0eff3645c385a8a~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="握手建立连接流程" style="zoom: 25%;" />

在连接建立之后，我们就可以使用`send()`发送数据，`recv()`接收数据。

**<u>（2）使用纯裸 TCP 会有什么问题</u>**

TCP 是有三个特点，**面向连接**、**可靠**、基于**字节流**。

- 今天我们需要关注的是 **基于字节流** 这一点。字节流可以理解为一个双向的通道里流淌的二进制数据，也就是 **01 串** 。纯裸 TCP 收发的这些 01 串之间是 **没有任何边界** 的，你根本不知道到哪个地方才算一条完整消息，这就是所谓的 **粘包问题**

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/b82d4fcdd0c4491e979856c93c1750d7~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="01二进制字节流" style="zoom:67%;" />

- 纯裸 TCP 是不能直接拿来用的，你需要在这个基础上加入一些 **自定义的规则** ，用于区分 **消息边界** 。于是我们会把每条要发送的数据都包装一下，比如加入 **消息头** ，消息头里写清楚一个完整的包长度是多少，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的 **消息体** 。
  - 而这里头提到的 **消息头** ，还可以放各种东西，比如消息体是否被压缩过和消息体格式之类的，只要上下游都约定好了，互相都认就可以了，这就是所谓的 **协议。**
  - 每个使用 TCP 的项目都可能会定义一套类似这样的协议解析标准，他们可能 **有区别，但原理都类似**。
  - **于是基于 TCP，就衍生了非常多的协议，比如 HTTP **

**<u>==（3）HTTP 和 RPC 有什么区别==</u>**

**<u>1）服务发现</u>**

​	首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道 **IP 地址和端口** 。这个找到服务对应的 IP 端口的过程，其实就是 **服务发现**。

- 在 **HTTP** 中，你知道服务的域名，就可以通过 **DNS 服务** 去解析得到它背后的 IP 地址，默认 **80 端口**；
- 而 **RPC** 的话，就有些区别，一般会有专门的中间服务去保存服务名和 IP 信息，比如 **Consul、Etcd、Nacos、ZooKeeper，甚至是 Redis**。想要访问某个服务，就去这些中间服务去获得 IP 和端口信息。由于 DNS 也是服务发现的一种，所以也有基于 DNS 去做服务发现的组件，比如 **CoreDNS**。

<u>**2）底层连接形式**</u>

- 以主流的 **HTTP1.1** 协议为例，其默认在建立底层 TCP 连接之后会一直保持这个连接（**keep alive**），之后的请求和响应都会复用这条连接。
- 而 **RPC** 协议，也跟 HTTP 类似，也是通过建立 TCP 长链接进行数据交互，但不同的地方在于，RPC 协议一般还会再建个 **连接池**，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，用完放回去，下次再复用。
- 由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给 HTTP 加个连接池，比如 Go 就是这么干的。

![connection_pool](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/72fcad064c9e4103a11f1a2d579f79b2~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png)

==<u><font color = '#8D0101'>**3）传输的内容**</font></u>==

基于 TCP 传输的消息，说到底，无非都是 **消息头 Header 和消息体 Body。**

- **Header** 是用于标记一些特殊信息，其中最重要的是 **消息体长度**。
- **Body** 是放我们真正需要传输的内容，而这些内容只能是二进制 01 串。所以 TCP 传字符串和数字都问题不大，因为字符串可以转成编码再变成 01 串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制 01 串，这样的方案现在也有很多现成的，比如 **Protocol Buffers (Protobuf)** 。

这个将结构体转为二进制数组的过程就叫 **序列化** ，反过来将二进制数组复原成结构体的过程叫 **反序列化**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/d501dfc6f764430188ce61fda0f3e5d9~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="序列化和反序列化" style="zoom:67%;" />

​	对于主流的 HTTP1.1，虽然它现在叫超文本协议，支持音频视频，但 HTTP 设计 初是用于做网页文本展示的，所以它传的内容以字符串为主。Header 和 Body 都是如此。在 Body 这块，它使用 **JSON** 来 **序列化** 结构体数据。==（json是一种文本类的序列化，可读性比较好，但是性能较差）==

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/04e8a79ddb7247759df23f1132c01655~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="HTTP报文" style="zoom:50%;" />

- 像 Header 里的那些信息，其实如果我们约定好头部的第几位是 `Content-Type`，就不需要每次都真的把 `Content-Type` 这个字段都传过来，类似的情况其实在 Body 的 JSON 结构里也特别明显。

- 而 RPC，因为它定制化程度更高，可以采用体积更小的 **Protobuf 或其他序列化协议去保存结构体数据**，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。**因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/284c26bb7f2848889d1d9b95cf49decb~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="HTTP原理" style="zoom:80%;" />

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/edb050d383c644e895e505253f1c4d90~tplv-k3u1fbpfcp-zoom-in-crop-mark%3A3024%3A0%3A0%3A0.awebp.png" alt="RPC原理" style="zoom:80%;" />

​	其实 **特指的是现在主流使用的 HTTP1.1**，`HTTP2`在前者的基础上做了很多改进，所以 **性能可能比很多 RPC 协议还要好**，甚至连`gRPC`底层都直接用的`HTTP2`。

**<u>4）总结</u>**

- 纯裸 TCP 是能收发数据，但它是个无边界的数据流，上层需要定义消息格式用于定义 **消息边界** 。于是就有了各种协议，HTTP 和各类 RPC 协议就是在 TCP 之上定义的应用层协议。
- **RPC 本质上不算是协议，而是一种调用方式**，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，**不一定非得基于 TCP 协议**。
- 从发展历史来说，**HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。** 很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。
- RPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP1.1 性能要更好，所以大部分公司内部都还在使用 RPC。
- **HTTP2.0** 在 **HTTP1.1** 的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。

### 对比Restful API和RPC

**<u>（1）RESTful API 架构</u>**

REST 最大的几个特点为：**资源、统一接口、URI 和无状态**。

**<u>1）资源</u>**

- 所谓"资源"，就是网络上的一个实体，或者说是网络上的一个具体信息。它可以是一段文本、一张图片、一首歌曲、一种服务，就是一个具体的实在。

**<u>2）统一接口</u>**

- RESTful 架构风格规定，数据的元操作，即 CRUD(Create，Read，Update 和 Delete，即数据的增删查改)操作，分别对应于 HTTP 方法：GET 用来获取资源，POST 用来新建资源（也可以用于更新资源），PUT 用来更新资源，DELETE 用来删除资源，这样就统一了数据操作的接口，仅通过 HTTP 方法，就可以完成对数据的所有增删查改工作。

<u>**3）URL**</u>

- 可以用一个 URI（统一资源定位符）指向资源，即每个 URI 都对应一个特定的资源。
- 要获取这个资源，访问它的 URI 就可以，因此 URI 就成了每一个资源的地址或识别符。

<u>**4）无状态**</u>

- 所谓无状态的，即所有的资源，都可以通过 URI 定位，而且这个定位与其他资源无关，也不会因为其他资源的变化而改变。有状态和无状态的区别，举个简单的例子说明一下。
  - 如查询员工的工资，如果查询工资是需要登录系统，进入查询工资的页面，执行相关操作后，获取工资的多少，则这种情况是有状态的。因为查询工资的每一步操作都依赖于前一步操作，只要前置操作不成功，后续操作就无法执行。
  - 如果输入一个 URI即可得到指定员工的工资，则这种情况是无状态的，因为获取工资不依赖于其他资源或状态。且这种情况下，员工工资是一个资源，由一个 URI与之对应，可以通过 HTTP 中的 GET 方法得到资源，这是典型的 RESTful 风格。

<u>**（2）RPC 和 Restful API 对比**</u>

**<u>1）面对对象不同</u>**

- RPC 更侧重于动作。
- REST 的主体是资源。

​	RESTful 是面向资源的设计架构，但在系统中有很多对象不能抽象成资源，比如登录，修改密码等而 RPC 可以通过动作去操作资源。所以在操作的全面性上 RPC 大于 RESTful。

**<u>2）传输效率</u>**

- RPC 效率更高。RPC，使用自定义的 TCP 协议，可以让请求报文体积更小，或者使用 HTTP2 协议，也可以很好的减少报文的体积，提高传输效率。

**<u>3）复杂度</u>**

- RPC 实现复杂，流程繁琐。
- REST 调用及测试都很方便。

​	RPC 实现需要实现编码，序列化，网络传输等。而 RESTful 不要关注这些，RESTful 实现更简单。

**<u>4）灵活性</u>**

- HTTP 相对更规范，更标准，更通用，无论哪种语言都支持 HTTP 协议。
- RPC 可以实现跨语言调用，但整体灵活性不如 RESTful。

**<u>（3）总结</u>**

- RPC 主要用于公司内部的服务调用，性能消耗低，传输效率高，实现复杂（大型的网站，内部子系统较多、接口非常多的情况下适合使用 RPC）。

- HTTP 主要用于对外的异构环境，浏览器接口调用，App 接口调用，第三方接口调用等。

# ZooKeeper

## ZK介绍

### ZooKeeper简介

​	ZooKeeper 是一个开源的**分布式协调服务**，它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。

> **原语：** 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性，即原语的执行必须是连续的，在执行过程中不允许被中断。

​	ZooKeeper 为我们提供了高可用、高性能、稳定的分布式数据一致性解决方案，通常被用于实现诸如**数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列**等功能。这些功能的实现主要依赖于 ZooKeeper 提供的 **数据存储+事件监听** 功能。

另外，很多顶级的开源项目都用到了 ZooKeeper，比如：

- **Kafka** : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。不过，在 Kafka 2.8 之后，引入了基于 Raft 协议的 KRaft 模式，不再依赖 Zookeeper，大大简化了 Kafka 的架构。
- **Hbase** : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。
- **Hadoop** : ZooKeeper 为 Namenode 提供高可用支持。

### ZK特性

- **顺序一致性：** 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。
- **原子性：** 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。
- **单一系统映像：** 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。
- **可靠性：** 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。
- **实时性：** 一旦数据发生变更，其他节点会实时感知到。每个客户端的系统视图都是最新的。
- **集群部署**：3~5 台（最好奇数台）机器就可以组成一个集群，每台机器都在内存保存了 ZooKeeper 的全部数据，机器之间互相通信同步数据，客户端连接任何一台机器都可以。
- **高可用：**如果某台机器宕机，会保证数据不丢失。集群中挂掉不超过一半的机器，都能保证集群可用。比如 3 台机器可以挂 1 台，5 台机器可以挂 2 台。

### ZooKeeper 应用场景

​	ZooKeeper 概览中，我们介绍到使用其通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。

1. **配置管理**：集群节点可以通过中心源获取启动配置；更简单的部署
2. **中心化和高可靠的数据注册**
3. **分布式集群管理**：节点加入/离开；节点的实时状态
4. **命名服务**：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID；DNS
5. **数据发布/订阅**：通过 **Watcher 机制** 可以很方便地实现数据发布/订阅。当你将数据发布到 ZooKeeper 被监听的节点上，其他机器可通过监听 ZooKeeper 上节点的变化来实现配置的动态更新。
6. **分布式锁**：通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。分布式锁的实现也需要用到 **Watcher 机制** 

实际上，这些功能的实现基本都得益于 ZooKeeper 可以保存数据的功能，但是 ZooKeeper 不适合保存大量数据，这一点需要注意。

## ZooKeeper 核心概念

### Data model（数据模型）

​	ZooKeeper 数据模型采用层次化的**多叉树形结构**，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二进制序列。并且。每个节点还可以拥有 N 个子节点，最上层是根节点以“/”来代表。每个数据节点在 ZooKeeper 中被称为 **znode**，它是 ZooKeeper 中**数据的最小单元**。并且，每个 znode 都有一个**唯一的路径标识**。

> [!NOTE]
>
> 强调一句：**ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的每个<font color = '#8D0101'>节点的数据大小上限是 1M</font> 。**

​	从下图可以更直观地看出：ZooKeeper 节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠"/"进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/znode-structure.png" alt="ZooKeeper 数据模型" style="zoom:70%;" />

### znode（数据节点）

​	介绍了 ZooKeeper 树形数据模型之后，我们知道每个数据节点在 ZooKeeper 中被称为 **znode**，它是 ZooKeeper 中数据的最小单元。你要存放的数据就放在上面，是你使用 ZooKeeper 过程中经常需要接触到的一个概念。

我们通常是将 znode 分为 4 大类：

- **持久（PERSISTENT）节点**：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。
- **临时（EPHEMERAL）节点**：临时节点的生命周期是与 **客户端会话（session）** 绑定的，**会话消失则节点消失**。并且，**临时节点只能做叶子节点** ，不能创建子节点。
- **持久顺序（PERSISTENT_SEQUENTIAL）节点**：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 `/node1/app0000000001`、`/node1/app0000000002` 。
- **临时顺序（EPHEMERAL_SEQUENTIAL）节点**：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性

每个 znode 由 2 部分组成:

- **stat**：状态信息
- **data**：节点存放的数据的具体内容

> [!NOTE]
>
> ZooKeeper 将**数据保存在内存中**，这也就保证了 **高吞吐量和低延迟**（但是**内存限制了能够存储的容量不太大**，此限制也是保持 znode 中存储的数据量较小的进一步原因）。

eg：通过 get 命令来获取 根目录下的 dubbo 节点的内容

```shell
[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo
# 该数据节点关联的数据内容为空
null
# 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出
cZxid = 0x2
ctime = Tue Nov 27 11:05:34 CST 2018
mZxid = 0x2
mtime = Tue Nov 27 11:05:34 CST 2018
pZxid = 0x3
cversion = 1
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 1
```

​	Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务 ID（cZxid）、节点创建时间（ctime） 和子节点个数（numChildren） 等等。

**znode 状态信息：**

| znode 状态信息 | 解释                                                         |
| -------------- | ------------------------------------------------------------ |
| cZxid          | create ZXID，即该数据节点被创建时的事务 id                   |
| ctime          | create time，即该节点的创建时间                              |
| mZxid          | modified ZXID，即该节点最终一次更新时的事务 id               |
| mtime          | modified time，即该节点最后一次的更新时间                    |
| pZxid          | 该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新 |
| cversion       | 子节点版本号，当前节点的子节点每次变化时值增加 1             |
| dataVersion    | 数据节点内容版本号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1 |
| aclVersion     | 节点的 ACL 版本号，表示该节点 ACL 信息变更次数               |
| ephemeralOwner | 创建该临时节点的会话的 sessionId；如果当前节点为持久节点，则 ephemeralOwner=0 |
| dataLength     | 数据节点内容长度                                             |
| numChildren    | 当前节点的子节点个数                                         |

### 版本（version）

在前面我们已经提到，对应于每个 znode，ZooKeeper 都会为其维护一个叫作 **Stat** 的数据结构，Stat 中记录了这个 znode 的三个相关的版本：

- **dataVersion**：当前 znode 节点的版本号
- **cversion**：当前 znode 子节点的版本
- **aclVersion**：当前 znode 的 ACL 的版本。

### ACL（权限控制）

ZooKeeper **采用 ACL（AccessControlLists）策略来进行权限控制**，类似于 UNIX 文件系统的权限控制。

对于 znode 操作的权限，ZooKeeper 提供了以下 5 种：

- **CREATE** : 能创建子节点
- **READ**：能获取节点数据和列出其子节点
- **WRITE** : 能设置/更新节点数据
- **DELETE** : 能删除子节点
- **ADMIN** : 能设置节点 ACL 的权限

其中尤其需要注意的是，**CREATE** 和 **DELETE** 这两种权限都是针对 **子节点** 的权限控制。

对于身份认证，提供了以下几种方式

- **world**：默认方式，所有用户都可无条件访问。
- **auth** :不使用任何 id，代表任何已认证的用户。
- **digest** :用户名:密码认证方式：*username:password* 。
- **ip** : 对指定 ip 进行限制。

### 事务

对于来自客户端的每个更新请求，ZooKeeper 具备严格的顺序访问控制能力。

**为了保证事务的顺序一致性，ZooKeeper 采用了<font color = '#8D0101'>递增</font>的事务 id 号（zxid）来标识事务**。

- **Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送**。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。**当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交**，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。
- 所有的提议（**`proposal`**）都在被提出的时候加上了 zxid。zxid 是一个 64 位的数字，它的高 32 位是 **`epoch`** 用来标识 Leader 关系是否改变，每次一个 Leader 被选出来，它都会有一个新的 epoch，标识当前属于那个 leader 的统治时期。低 32 位用于递增计数。

详细过程如下：

1. Leader 等待 Server 连接；
2. Follower 连接 Leader，将最大的 zxid 发送给 Leader；
3. Leader 根据 Follower 的 zxid 确定同步点；
4. 完成同步后通知 follower 已经成为 uptodate 状态；
5. Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了。

### ==Watcher（事件监听器）==

​	Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户（客户端）在指定节点上注册一些 Watcher，**监听它关心的 znode，当 znode 状态发生变化（数据变化、子节点增减变化）时，ZooKeeper 服务会通知客户端**。该机制是 ZooKeeper 实现分布式协调服务的重要特性。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/zookeeper-watcher.png" alt="ZooKeeper Watcher 机制" style="zoom:67%;" />

客户端和服务端保持连接一般有两种形式：

- **客户端向服务端不断轮询**
- **服务端向客户端推送状态**

Zookeeper 的选择是服务端主动推送状态，也就是**观察机制（ `Watch` ）**。

ZooKeeper 的观察机制允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。

- 监听器实时触发
- 监听器总是有序的
- 创建新的 znode 数据前，客户端就能收到监听事件。

**eg：**

客户端使用 `getData` 等接口获取 znode 状态时传入了一个用于处理节点变更的回调，那么服务端就会主动向客户端推送节点的变更：

```java
public byte[] getData(final String path, Watcher watcher, Stat stat)
```

从这个方法中传入的 `Watcher` 对象实现了相应的 `process` 方法，每次对应节点出现了状态的改变，`WatchManager` 都会通过以下的方式调用传入 `Watcher` 的方法：

```java
Set<Watcher> triggerWatch(String path, EventType type, Set<Watcher> supress) {
    WatchedEvent e = new WatchedEvent(type, KeeperState.SyncConnected, path);
    Set<Watcher> watchers;
    synchronized (this) {
        watchers = watchTable.remove(path);
    }
    for (Watcher w : watchers) {
        w.process(e);
    }
    return watchers;
}
```

​	Zookeeper 中的所有数据其实都是由一个名为 `DataTree` 的数据结构管理的，所有的读写数据的请求最终都会改变这颗树的内容，**在发出读请求时可能会传入 `Watcher` 注册一个回调函数，而写请求就可能会触发相应的回调，由 `WatchManager` 通知客户端数据的变化**。

​	通知机制的实现其实还是比较简单的，**通过读请求设置 `Watcher` 监听事件，写请求在触发事件时就能将通知发送给指定的客户端**。

### 会话（Session）

​	Session 可以看作是 **ZooKeeper 服务器与客户端的之间的一个 TCP 长连接**，通过这个连接，客户端能够通过**心跳检测**与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。

- 每个 ZooKeeper 客户端配置中都配置了 ZooKeeper 服务器集群列表。启动时，客户端会遍历列表去尝试建立连接。如果失败，它会尝试连接下一个服务器，依次类推。

ZooKeeper 的会话具有四个属性：

- **`sessionID` - 会话 ID**，唯一标识一个会话
  - 每次客户端创建新的会话时，Zookeeper 都会为其分配一个全局唯一的 sessionID。 `sessionID`是 ZooKeeper 会话的一个重要标识
  - 许多与会话相关的运行机制都是基于这个 `sessionID` 的，因此无**论是哪台服务器为客户端分配的 `sessionID`，都务必保证全局唯一**。
- **`TimeOut` - 会话超时时间**
  - 客户端在构造 Zookeeper 实例时，会配置 sessionTimeout 参数用于指定会话的超时时间，Zookeeper 客户端向服务端发送这个超时时间后，服务端会根据自己的超时时间限制最终确定会话的超时时间。
  - 当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在`sessionTimeout`规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。
  - 一旦会话过期，就无法再重新打开，且任何与该会话相关的临时 znode 都会被删除。
- **`TickTime` - 下次会话超时时间点**
  - 为了便于 Zookeeper 对会话实行”分桶策略”管理，同时为了高效低耗地实现会话的超时检查与清理，Zookeeper 会为每个会话标记一个下次会话超时时间点，其值大致等于当前时间加上 TimeOut。
- **`isClosing` - 标记一个会话是否已经被关闭**
  - 当服务端检测到会话已经超时失效时，会将该会话的 isClosing 标记为”已关闭”，这样就能确保不再处理来自该会话的新情求了。

​	Zookeeper 的会话管理主要是通过 `SessionTracker` 来负责，其采用了**分桶策略**（将类似的会话放在同一区块中进行管理）进行管理，以便 Zookeeper 对会话进行不同区块的隔离处理以及同一区块的统一处理。

## ZooKeeper 集群

### 集群架构以及<font color = '#8D0101'>数据副本</font>

​	为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。ZooKeeper 官方提供的架构图就是一个 ZooKeeper 集群整体对外提供服务。

![ZooKeeper 集群架构](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/zookeeper-cluster.png)

​	上图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 **ZAB 协议**（ZooKeeper Atomic Broadcast）来保持数据的一致性。

​	**Zookeeper 集群是一个基于主从复制的高可用集群，集群中每个节点都存储了一份==数据副本==（内存中）。**

> [!NOTE]
>
> 1）ZooKeeper的主从架构是指**多台机器之间的主从关系** ，而非单台机器上的节点。
>
> 2）ZooKeeper 的**数据副本**指的是**集群中每个节点**（包括主节点 Leader 和从节点 Follower/Observer）在内存中实时维护的**同一份数据的完整拷贝** 。
>
> - **数据内容**
>   数据副本包含 ZooKeeper 的**全量数据树** （类似文件系统的树形结构），每个节点（znode）的元数据（如版本、ACL 权限等）以及存储的数据内容（最大 1MB）。例如，服务注册信息、分布式锁状态等均会被所有节点完整保存。
> - **存储形式**
>   数据以**内存数据库** 的形式存在，保证低延迟的读写性能。同时，ZooKeeper 会通过**事务日志** 和**快照文件** 定期将内存数据持久化到磁盘，但实时服务依赖内存中的副本。
> - **一致性保障**
>   主节点（Leader）处理所有写请求，并通过 ZAB 协议将数据变更同步到所有从节点（Follower），确保集群中每个节点的内存副本**强一致性** 。客户端无论连接到哪个节点，读取的数据都是最新的。
> - **与客户端连接的关系**
>   客户端通过 TCP 长连接（会话）访问某个节点时，该节点直接使用内存中的数据副本响应请求，无需跨节点查询。若客户端连接的节点故障，会自动重连到其他节点，因数据副本一致，服务不受影响。
>
> 3）数据副本是 ZooKeeper 实现**高可用和强一致性的核心机制**，通过主从复制确保所有节点内存中存储相同的数据状态，客户端访问时直接利用本地副本提供服务。

​	客户端可以从任意 ZooKeeper 服务器节点读取数据，但**只能通过 Leader 服务写数据并需要<font color = '#8D0101'>半数以上 Follower 的 ACK</font>**，才算写入成功。

### ZooKeeper 集群角色

​	在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了 Leader、Follower 和 Observer 三种角色。如下图所示：

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/zookeeper-cluser-roles.png" alt="ZooKeeper 集群中角色" style="zoom:80%;" />

​	ZooKeeper 集群中的所有机器通过一个 **Leader 选举过程** 来选定一台称为 “**Leader**” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，**Follower** 和 **Observer** 都只能提供读服务。Follower 和 Observer 唯一的区别在于 **Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略**，因此 Observer 机器可以在**不影响写性能的情况下提升集群的读性能**。

| 角色     | 说明                                                         |
| -------- | ------------------------------------------------------------ |
| Leader   | 为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 |
| Follower | 为客户端提供读服务，如果是写服务则转发给 Leader。参与选举过程中的投票。 |
| Observer | 为客户端提供读服务，如果是**写服务则转发给 Leader**。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。 |

### 集群读写操作

**<u>（1）读操作</u>**

**Leader/Follower/Observer 都可直接处理读请求，从本地内存中读取数据并返回给客户端即可**。

由于处理读请求不需要服务器之间的交互，**Follower/Observer 越多，整体系统的读请求吞吐量越大**，也即读性能越好。

![image-20251018233430056](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/image-20251018233430056.png)

**<u>（2） 写操作</u>**

​	所有的写请求实际上**都要交给 Leader 处理**。Leader 将写请求以事务形式发给所有 Follower 并等待 ACK，一旦收到半数以上 Follower 的 ACK，即认为写操作成功。

**<u>1）写 Leader</u>**

由下图可见，通过 Leader 进行写操作，主要分为五步：

1. 客户端向 Leader 发起写请求
2. Leader 将写请求以事务 Proposal 的形式发给所有 Follower 并等待 ACK
3. Follower 收到 Leader 的事务 Proposal 后返回 ACK
4. Leader 得到过半数的 ACK（Leader 对自己默认有一个 ACK）后向所有的 Follower 和 Observer 发送 Commmit
5. Leader 将处理结果返回给客户端

> [!NOTE]
>
> Leader 不需要得到 Observer 的 ACK，即 Observer 无投票权。
>
> Leader 不需要得到所有 Follower 的 ACK，只要收到过半的 ACK 即可，同时 Leader 本身对自己有一个 ACK。上图中有 4 个 Follower，只需其中两个返回 ACK 即可，因为 $$(2+1) / (4+1) > 1/2$$ 。
>
> Observer 虽然无投票权，但仍须同步 Leader 的数据从而在处理读请求时可以返回尽可能新的数据。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/zookeeper_4.png" alt="img" style="zoom:55%;" />

**<u>2）写 Follower/Observer</u>**

![image-20251018233520841](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/image-20251018233520841.png)

- Follower/Observer 均可接受写请求，但不能直接处理，而需要将写请求转发给 Leader 处理。
- 除了多了一步请求转发，其它流程与直接写 Leader 无任何区别。

## ZAB协议

### ZAB 协议介绍

**<u>（1）ZAB 协议简介</u>**

​	作为一个优秀高效且可靠的分布式协调框架，`ZooKeeper` 在解决分布式数据一致性问题时并没有直接使用 `Paxos` ，而是专门定制了一致性协议叫做 `ZAB(ZooKeeper Automic Broadcast)` 原子广播协议，该协议能够很好地支持 **崩溃恢复** 。

​	ZAB（ZooKeeper Atomic Broadcast，原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。

<u>**（2）ZAB 协议两种基本的模式：崩溃恢复和消息广播**</u>

ZAB 协议包括两种基本的模式，分别是

- **崩溃恢复**：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，**所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致**。
- **消息广播**：**当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。** 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。

### ZooKeeper 集群 Leader 选举（崩溃恢复）

> [!IMPORTANT]
>
> **ZooKeeper 的故障恢复**
>
> ZooKeeper 集群采用一主（称为 Leader）多从（称为 Follower）模式，主从节点通过副本机制保证数据一致。
>
> - **如果 Follower 节点挂了** - ZooKeeper 集群中的每个节点都会单独在内存中维护自身的状态，并且各节点之间都保持着通讯，**只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务**。
> - **如果 Leader 节点挂了** - 如果 Leader 节点挂了，系统就不能正常工作了。此时，需要通过 ZAB 协议的选举 Leader 机制来进行故障恢复。
>
> ZAB 协议的选举 Leader 机制简单来说，就是：基于过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出选举 Leader 模式，进入原子广播模式。

当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入 Leader 选举过程，这个过程会选举产生新的 Leader 服务器。

<u>**（1）术语**</u>

- **myid** - 每个 Zookeeper 服务器，都需要在数据文件夹下创建一个名为 myid 的文件，该文件包含整个 Zookeeper 集群唯一的 ID（整数）。
- **zxid** - 类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal ID。为了保证顺序性，该 zkid 必须单调递增。因此 Zookeeper 使用一个 64 位的数来表示，高 32 位是 Leader 的 epoch，从 1 开始，每次选出新的 Leader，epoch 加一。低 32 位为该 epoch 内的序号，每次 epoch 变化，都将低 32 位的序号重置。这样保证了 zkid 的全局递增性。

**<u>（2）服务器状态</u>**

- **LOOKING** - 不确定 Leader 状态。该状态下的服务器认为当前集群中没有 Leader，会发起 Leader 选举
- **FOLLOWING** - 跟随者状态。表明当前服务器角色是 Follower，并且它知道 Leader 是谁
- **LEADING** - 领导者状态。表明当前服务器角色是 Leader，它会维护与 Follower 间的心跳
- **OBSERVING** - 观察者状态。表明当前服务器角色是 Observer，与 Folower 唯一的不同在于不参与选举，也不参与集群写操作时的投票

**<u>（3）选举过程</u>**

1. **Leader election（选举阶段）**：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。
2. **Discovery（发现阶段）**：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。
3. **Synchronization（同步阶段）**：同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后准 leader 才会成为真正的 leader。
4. **Broadcast（广播阶段）**：到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。

**<u>（4）ZooKeeper 集群为啥最好奇数台？（节省资源）</u>**

​	ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。

- 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。
- 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。

**<u>（5）ZooKeeper 选举的过半机制防止脑裂</u>**

**何为集群脑裂？**

对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。

举例说明：比如现在有一个由 6 台服务器所组成的一个集群，部署在了 2 个机房，每个机房 3 台。正常情况下只有 1 个 leader，但是当两个机房中间网络断开的时候，每个机房的 3 台服务器都会认为另一个机房的 3 台服务器下线，而选出自己的 leader 并对外提供服务。若没有过半机制，当网络恢复的时候会发现有 2 个 leader。仿佛是 1 个大脑（leader）分散成了 2 个大脑，这就发生了脑裂现象。脑裂期间 2 个大脑都可能对外提供了服务，这将会带来数据一致性等问题。

**过半机制是如何防止脑裂现象产生的？**

ZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。

### 原子（消息）广播（Atomic Broadcast）

说白了就是 `ZAB` 协议是**<font color = '#8D0101'>如何处理写请求的</font>**

只有 `Leader` 能处理写请求，那么我们的 `Follower` 和 `Observer` 也需要 **同步更新数据** 

- **所有的写请求都会被转发给 Leader，Leader 会以原子广播的方式通知 Follow。当半数以上的 Follow 已经更新状态持久化后，Leader 才会提交这个更新，然后客户端才会收到一个更新成功的响应**。这有些类似数据库中的两阶段提交协议。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/b64c7f25a5d24766889da14260005e31~tplv-k3u1fbpfcp-zoom-1.jpeg" alt="消息广播" style="zoom:65%;" />

**这两个 `Queue` 的作用**：

- 答案是 `ZAB` 需要让 `Follower` 和 `Observer` 保证**顺序性** 。何为顺序性，比如我现在有一个写请求 A，此时 `Leader` 将请求 A 广播出去，因为只需要半数同意就行，所以可能这个时候有一个 `Follower` F1 因为网络原因没有收到，而 `Leader` 又广播了一个请求 B，因为网络原因，F1 竟然先收到了请求 B 然后才收到了请求 A，这个时候**请求处理的顺序不同就会导致数据的不同**，从而 **产生数据不一致问题** 。
  - 所以在 `Leader` 这端，它为每个其他的 `zkServer` 准备了一个 **队列** ，采用先进先出的方式发送消息。由于协议是 **通过 `TCP`** 来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。
  - 除此之外，在 `ZAB` 中还定义了一个 **全局单调递增的事务 ID `ZXID`** ，它是一个 64 位 long 型，其中高 32 位表示 `epoch` 年代，低 32 位表示事务 id。`epoch` 是会根据 `Leader` 的变化而变化的，当一个 `Leader` 挂了，新的 `Leader` 上位的时候，年代（`epoch`）就变了。而低 32 位可以简单理解为递增的事务 id。（定义这个的原因也是为了顺序性，每个 `proposal` 在 `Leader` 中生成后需要 **通过其 `ZXID` 来进行排序** ，才能得到处理。）

## ZooKeeper 应用

### 命名服务

如何给一个对象设置ID，大家可能都会想到 `UUID`，但是 `UUID` 最大的问题就在于它太长了

在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等

- 我们之前提到过 `zookeeper` 是通过 **树形结构** 来存储数据节点的，那也就是说，对于每个节点的 **全路径，它必定是唯一的**，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20200602182548.png" alt="img" style="zoom:50%;" />

### 选主

因为 `Zookeeper` 的强一致性，能够很好地在保证 **在高并发的情况下保证节点创建的全局唯一性** (即无法重复创建同样的节点)。

利用这个特性，我们可以 **让多个客户端创建一个指定的节点** ，创建成功的就是 `master`。

但是，如果这个 `master` 挂了怎么办？

- 你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？`master` 挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得 `watcher` 吗？我们是不是可以 **让其他不是 `master` 的节点监听节点的状态** ，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表 `master` 挂了，这个时候我们 **触发回调函数进行重新选举** ，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断 `master` 是否挂了等等。

​	总的来说，我们可以完全 **利用 临时节点、节点状态 和 `watcher` 来实现选主的功能**，临时节点主要用来选举，节点状态和`watcher` 可以用来判断 `master` 的活性和进行重新选举。

### 集群管理和注册中心

​	可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对集群中的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。而 `zookeeper` 天然支持的 **`watcher` 和 临时节点**能很好的实现这些需求。

**<u>（1）集群管理</u>**

- 我们可以为每台机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 `watcher` 进行状态监控和回调

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16f8ed584372a0d2~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.awebp" alt="集群管理" style="zoom:50%;" />

**<u>（2）注册中心</u>**

- 我们同样也是让 **服务提供者** 在 `zookeeper` 中创建一个临时节点并且将自己的 `ip、port、调用方式` 写入节点，当 **服务消费者** 需要进行调用的时候会 **通过注册中心找到相应的服务的地址列表(IP端口什么的)** ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务

- 当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 `Eureka` 会先试错，然后再更新）。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/16f8ed584e3bef3a~tplv-t2oaga2asx-jj-mark%3A3024%3A0%3A0%3A0%3Aq75.awebp" alt="注册中心" style="zoom:50%;" />

### 分布式锁

**可查看分布式-CAP实际应用案例-分布式锁选择AP or CP-基于zookeeper实现分布式锁-CP**

## 总结、与ETCD的对比

**<u>（1）总结</u>**

1. ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。
2. 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。
3. ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 znode 中存储的数据量较小的进一步原因）。
4. ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）
5. ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 znode 被创建了，除非主动进行 znode 的移除操作，否则这个 znode 将一直保存在 ZooKeeper 上。
6. **ZooKeeper 底层其实只提供了两个功能：==① 管理（存储、读取）用户程序提交的数据；② 为用户程序提供数据节点监听服务==**。

**<u>（2）与ETCD的对比</u>**

​	[ETCD](https://etcd.io/) 是一种强一致性的分布式键值存储，它提供了一种可靠的方式来存储需要由分布式系统或机器集群访问的数据。ETCD 内部采用 [Raft 算法](https://javaguide.cn/distributed-system/protocol/raft-algorithm.html)作为一致性算法，基于 Go 语言实现。

与 ZooKeeper 类似，ETCD 也可用于数据发布/订阅、负载均衡、命名服务、分布式协调/通知、分布式锁等场景

|                  |                          ZooKeeper                           |                          ETCD                          |
| ---------------- | :----------------------------------------------------------: | :----------------------------------------------------: |
| **语言**         |                             Java                             |                           Go                           |
| **协议**         |                             TCP                              |                          Grpc                          |
| **接口调用**     |               必须要使用自己的 client 进行调用               |     可通过 HTTP 传输，即可通过 CURL 等命令实现调用     |
| **一致性算法**   |                           Zab 协议                           |                       Raft 算法                        |
| **Watcher 机制** |                     较局限，一次性触发器                     |             一次 Watch 可以监听所有的事件              |
| **数据模型**     |                      基于目录的层次模式                      |        参考了 zk 的数据模型，是个扁平的 kv 模型        |
| **存储**         | kv 存储，使用的是 ConcurrentHashMap，内存存储，一般不建议存储较多数据 | kv 存储，使用 bbolt 存储引擎，可以处理几个 GB 的数据。 |
| **MVCC**         |                            不支持                            |          支持，通过两个 B+ Tree 进行版本控制           |
| **全局 Session** |                           存在缺陷                           |              实现更灵活，避免了安全性问题              |
| **权限校验**     |                             ACL                              |                          RBAC                          |
| **事务能力**     |                     提供了简易的事务能力                     |                只提供了版本号的检查能力                |
| **部署维护**     |                             复杂                             |                          简单                          |

​	ZooKeeper 在存储性能、全局 Session、Watcher 机制等方面存在一定局限性，越来越多的开源项目在替换 ZooKeeper 为 Raft 实现或其它分布式协调服务，例如：[Kafka Needs No Keeper - Removing ZooKeeper Dependency (confluent.io)](https://www.confluent.io/blog/removing-zookeeper-dependency-in-kafka/)、[Moving Toward a ZooKeeper-Less Apache Pulsar (streamnative.io)](https://streamnative.io/blog/moving-toward-zookeeper-less-apache-pulsar)。

​	ETCD 相对来说更优秀一些，提供了更稳定的高负载读写能力，对 ZooKeeper 暴露的许多问题进行了改进优化。并且，ETCD 基本能够覆盖 ZooKeeper 的所有应用场景，实现对其的替代。



# Kafka

## Kafka基础

### Kafka 是什么？主要应用场景有哪些？

Kafka 是一个分布式流式处理平台。这到底是什么意思呢？

流平台具有三个关键功能：

1. **消息队列**：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。
2. **容错的持久方式存储记录消息流**：Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。
3. **流式处理平台：** 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。

Kafka 主要有两大应用场景：

1. **消息队列**：建立实时流数据管道，以可靠地在系统或应用程序之间获取数据。
2. **数据处理：** 构建实时的流数据处理程序来转换或处理数据流。

### 和其他消息队列相比,Kafka 的优势在哪里

​	我们现在经常提到 Kafka 的时候就已经默认它是一个非常优秀的消息队列了，我们也会经常拿它跟 RocketMQ、RabbitMQ 对比。我觉得 Kafka 相比其他消息队列主要的优势如下：

1. **极致的性能**：基于 Scala 和 Java 语言开发，设计中大量使用了批量处理和异步的思想，最高可以每秒处理千万级别的消息。
2. **生态系统兼容性无可匹敌**：Kafka 与周边生态系统的兼容性是最好的没有之一，尤其在大数据和流计算领域。

​	实际上在早期的时候 Kafka 并不是一个合格的消息队列，早期的 Kafka 在消息队列领域就像是一个衣衫褴褛的孩子一样，功能不完备并且有一些小问题比如丢失消息、不保证消息可靠性等等。当然，这也和 LinkedIn 最早开发 Kafka 用于处理海量的日志有很大关系，哈哈哈，人家本来最开始就不是为了作为消息队列滴，谁知道后面误打误撞在消息队列领域占据了一席之地。

随着后续的发展，这些短板都被 Kafka 逐步修复完善。所以，**Kafka 作为消息队列不可靠这个说法已经过时！**

### 队列模型、Kafka 的消息模型是什么？

<u>**（1）队列模型点对点：早期的消息模型**</u>

![队列模型](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/%E9%98%9F%E5%88%97%E6%A8%A1%E5%9E%8B23.png)

​	**使用队列（Queue）作为消息通信载体，满足生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。** 比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）

**队列模型存在的问题：**

- 假如我们存在这样一种情况：我们需要将生产者产生的消息分发给多个消费者，并且每个消费者都能接收到完整的消息内容。
- 这种情况，队列模型就不好解决了。很多比较杠精的人就说：我们可以为每个消费者创建一个单独的队列，让生产者发送多份。这是一种非常愚蠢的做法，浪费资源不说，还违背了使用消息队列的目的。

**<u>（2）发布-订阅模型:Kafka 消息模型</u>**

发布-订阅模型主要是为了解决队列模型存在的问题。

![发布订阅模型](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%9E%8B.png)

发布订阅模型（Pub-Sub） 使用**主题（Topic）** 作为消息通信载体，类似于**广播模式**；发布者发布一条消息，该消息通过主题传递给所有的订阅者，**在一条消息广播之后才订阅的用户则是收不到该条消息的**。

**在发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。所以说，发布 - 订阅模型在功能层面上是可以兼容队列模型的。**

**Kafka 采用的就是发布 - 订阅模型。**

> **RocketMQ 的消息模型和 Kafka 基本是完全一样的。唯一的区别是 Kafka 中没有队列这个概念，与之对应的是 Partition（分区）。**

## Kafka核心概念

### Kafka 术语

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/58c35d3ab0921bf0476e3ba14069d291.jpg" alt="img" style="zoom:20%;" />

- **消息（Messages）**：Kafka 的数据单元被称为消息。**消息由字节数组组成**。
- **批次（Batches）**：批次就是一组消息，这些消息属于同一个主题和分区。
- **主题（Topic）**：Kafka 消息通过主题进行分类。主题就类似**数据库的表**。
  - 不同主题的消息是**物理隔离**的；
  - **同一个主题的消息保存在一个或多个 Broker 上**。但用户只需指定消息的 Topic 即可生产或消费数据而**不必关心数据存于何处**。
  - **主题有一个或多个分区**。
- **分区（Partition）**：分区是一个**有序不变的消息序列**，消息以**追加**的方式写入分区，然后以**先入先出**的顺序读取。Kafka 通过分区来实现数据冗余和伸缩性，**<font color = '#8D0101'>实际上可以对应成为消息队列中的队列</font>**
- **消息偏移量（Offset）**：表示**分区（Partition）中**每条消息的**位置信息**，是一个**单调递增且不变的值**。
- **生产者（Producer）**：生产者是向主题发布新消息的 Kafka 客户端。生产者可以**将数据发布到所选择的主题（Topic）**中。生产者负责将**记录分配到主题（Topic）中的哪一个分区（Partition）中**。
- **消费者（Consumer）**：消费者是从主题订阅新消息的 Kafka 客户端。**消费者通过检查消息的偏移量来区分消息是否已读**。
- **消费者群组（Consumer Group）**：多个消费者共同构成的一个群组，<font color = '#8D0101'>**同时消费多个分区以实现高并发**</font>。
  - 每个消费者属于一个特定的消费者群组（可以为每个消费者指定消费者群组，若不指定，则属于默认的群组）。
  - 群组中，一个消费者**可以消费多个分区**
  - 群组中，**每个分区只能被指定给一个消费**
- **再均衡（Rebalance）**：消费者组内**某个消费者实例挂掉**后，其他消费者实例自动重新分配订阅主题分区的过程。<font color = '#8D0101'>**分区再均衡是 Kafka 消费者端实现高可用的重要手段**</font>。
- **Broker** ：**一个独立的 Kafka 服务器被称为 Broker**。Broker 接受来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存；消费者向 Broker 请求消息，Broker 负责返回已提交的消息。
- **集群（Cluster）**：Broker 是集群 (Cluster) 的组成部分。每一个集群都会选**举出一个 Broker 作为集群控制器 (Controller)**，集群控制器负责管理工作，包括将分区分配给 Broker 和监控 Broker。
  - 在集群中，**一个分区 (Partition) 从属一个 Broker，该 Broker 被称为分区的首领 (Leader)**。一个分区可以分配给多个 Brokers，这个时候会发生**分区复制**。这种复制机制为分区提供了消息冗余，如果有一个 Broker 失效，其他 Broker 可以接管领导权。

- **副本（Replica）**：**Partition 的副本，保障 Partition 的高可用**。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为==领导者副本和追随者副本==，各自有不同的角色划分。**<font color = '#8D0101'>副本是在分区层级下的，即每个分区可配置多个副本实现高可用</font>**。
- **Leader：** **Replicas 中的一个角色**， **Producer 和 Consumer 只与 Leader 交互**；
- **Follower：** **Replicas 中的一个角色**，从 Leader 中复制数据，作为它的副本，同时一旦某 Leader 挂掉，便会从它的所有 Follower 中选举出一个新的 Leader 继续提供服务；
- **Controller：** Kafka 集群中的一个服务器，它的主要作用是在 ZooKeeper 的帮助下管理和协调整个 Kafka 集群。控制器其实就是一个 Broker，只不过它除了具有一般 Broker 的功能以外，还负责 Leader 的选举。
- **Zookeeper：** Kafka 通过 ZooKeeper 存储集群的 Meta 信息等。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6b61666b612d636c75737465722e706e67.png" alt="img" style="zoom:75%;" />

### 详细解释Producer、Consumer、Broker、Topic、Partition

Kafka 将生产者发布的消息发送到 **Topic（主题）** 中，需要这些消息的消费者可以订阅这些 **Topic（主题）**，如下图所示：

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/message-queue20210507200944439.png" alt="img" style="zoom:67%;" />

1. **Producer（生产者）** : 产生消息的一方。
2. **Consumer（消费者）** : 消费消息的一方。
3. **Broker（代理）** : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。

同时，你一定也注意到每个 Broker 中又包含了 Topic 以及 Partition 这两个重要的概念：

- **Topic（主题）** : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。
- **Partition（分区）** : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 上，这也就表明一个 **Topic 可以横跨多个 Broker** 。这正如我上面所画的图一样。

> 划重点：**Kafka 中的 Partition（分区） 实际上可以对应成为消息队列中的队列。这样是不是更好理解一点？**

### Kafka 的多副本机制是什么，带来了什么好处

​	 Kafka 为**分区（Partition）引入了多副本（Replica）机制**。分区（Partition）中的多个副本之间会有一个叫做 leader 的家伙，其他副本称为 follower。我们发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。

> 生产者和消费者只与 leader 副本交互。你可以理解为**其他副本只是 leader 副本的拷贝**，它们的存在只是为了保证消息存储的安全性。当 leader 副本发生故障时会从 follower 中选举出一个 leader,但是 follower 中如果有和 leader 同步程度达不到要求的参加不了 leader 的竞选。

**Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？**

1. Kafka 通过给特定 Topic 指定多个 Partition, 而各个 Partition 可以分布在不同的 Broker 上, 这样便能提供比较好的并发能力（负载均衡）。
2. Partition 可以指定对应的 Replica 数, 这也极大地提高了消息存储的安全性, 提高了容灾能力，不过也相应的增加了所需要的存储空间。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6b61666b612d636c75737465722e706e67.png" alt="img" style="zoom:75%;" />

## Kafka 生产者

###  生产者发送流程

**<u>1）生产者传输实体</u>**

Kafka Producer 发送的**数据对象叫做 `ProducerRecord`** ，它有 4 个关键参数：

- `Topic` - 主题
- `Partition` - 分区（非必填）
- `Key` - 键（非必填）
- `Value` - 值

**<u>2）Kafka 生产者发送消息流程</u>**

- **序列化** - 发送前，生产者要先**把键和值序列化**。

- **分区** - 数据被传给分区器。如果在 `ProducerRecord` 中已经指定了分区，那么分区器什么也不会做；否则，分区器会根据 `ProducerRecord` 的键来选择一个分区。选定分区后，生产者就知道该把消息发送给哪个主题的哪个分区。

- **批次传输** - 接着，这条记录会被添加到一个记录批次中。**这个批次中的所有消息都会被发送到相同的主题和分区上**。有一个独立的线程负责将这些记录批次发送到相应 Broker 上。
  - **批次，就是一组消息，这些消息属于同一个主题和分区**。

  - 发送时，会把消息分成**批次传输**，如果每次只发送一个消息，会占用大量的网路开销。

- **响应** - 服务器收到消息会返回一个响应。

  - 如果**成功**，则返回一个 `RecordMetaData` 对象，它包含了**主题、分区、偏移量**；

  - 如果**失败**，则返回一个错误。生产者在收到错误后，可以进行重试，重试次数可以在配置中指定。失败一定次数后，就返回错误消息。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20200528224323.png" alt="img" style="zoom:55%;" />

<u>**3）生产者向 Broker 发送消息时是怎么确定向哪一个 Broker 发送消息？**</u>

- 生产者会向任意 broker 发送一个元数据请求（`MetadataRequest`），获取到每一个分区对应的 Leader 信息，并缓存到本地。
- 生产者在发送消息时，会指定 Partition 或者通过 key 得到一个 Partition，然后根据 Partition 从缓存中获取相应的 Leader 信息。

### 生产者 API

生产者 API 允许应用程序将数据流发送到 Kafka 集群中的主题。[javadocs](https://kafka.apache.org/39/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html) 中给出了展示如何使用生产者的示例 。要使用生产者，可以使用以下 Maven 依赖项：

```xml
<dependency>
	<groupId>org.apache.kafka</groupId>
	<artifactId>kafka-clients</artifactId>
	<version>3.9.0</version>
</dependency>
```

Kafka 的 Java 生产者 API 主要的对象就是 `KafkaProducer`。通常我们开发一个生产者的步骤有 4 步。

1. 构造生产者对象所需的参数对象。
2. 利用第 1 步的参数对象，创建 `KafkaProducer` 对象实例。
3. 使用 `KafkaProducer` 的 `send` 方法发送消息。
4. 调用 `KafkaProducer` 的 `close` 方法关闭生产者并释放各种系统资源

**<u>（1）创建生产者</u>**

Kafka 生产者核心配置：

- `bootstrap.servers` - 指定了 Producer 启动时要连接的 Broker 地址。注：如果你指定了 1000 个 Broker 连接信息，那么，Producer 启动时就会首先创建与这 1000 个 Broker 的 TCP 连接。在实际使用过程中，并不建议把集群中所有的 Broker 信息都配置到 `bootstrap.servers` 中，通常你指定 3 ～ 4 台就足以了。因为 Producer 一旦连接到集群中的任一台 Broker，就能拿到整个集群的 Broker 信息，故没必要为 `bootstrap.servers` 指定所有的 Broker。
- `key.serializer` - 键的序列化器。
- `value.serializer` - 值的序列化器。

```java
// 指定生产者的配置
final Properties properties = new Properties();
properties.put("bootstrap.servers", "localhost:9092");
// 设置 key 的序列化器
properties.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
// 设置 value 的序列化器
properties.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

// 使用配置初始化 Kafka 生产者
Producer<String, String> producer = new KafkaProducer<>(properties);
```

**<u>（2）异步发送</u>**

直接发送消息，不关心消息是否到达；这种方式吞吐量最高，但有小概率会丢失消息。

```java
ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
try {
    producer.send(record);
} catch (Exception e) {
    e.printStackTrace();
}
```

**<u>（3）同步发送</u>**

返回一个 `Future` 对象，调用 `get()` 方法，会一直阻塞等待 `Broker` 返回结果；这是一种可靠传输方式，但吞吐量最差。

```java
ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
try {
    producer.send(record).get();
} catch (Exception e) {
    e.printStackTrace();
}
```

**<u>（4）异步响应发送</u>**

异步方式相对于“发送并忽略返回”的方式的不同在于：在异步返回时可以执行一些操作，如：抛出异常、记录错误日志；这是一个折中的方案，即兼顾吞吐量，也保证消息不丢失。

- 首先，定义一个 callback：

```java
private class DemoProducerCallback implements Callback {
      @Override
        public void onCompletion(RecordMetadata recordMetadata, Exception e) {
           if (e != null) {
               e.printStackTrace();
             }
        }
}
```

- 然后，使用这个 callback：

```java
ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Biomedical Materials", "USA");
producer.send(record, new DemoProducerCallback());
```

**<u>（5）关闭连接</u>**

调用 `producer.close()` 方法可以关闭 Kafka 生产者连接。

```java
Producer<String, String> producer = new KafkaProducer<>(properties);
try {
   producer.send(new ProducerRecord<>(topic, msg));
} catch (Exception e) {
    e.printStackTrace();
} finally {
    // 关闭连接
    producer.close();
}
```

### 生产者的连接

**Apache Kafka 的所有通信都是基于 TCP 的**。无论是生产者、消费者，还是 Broker 之间的通信都是如此。选用 TCP 连接是由于 TCP 本身提供的一些高级功能，如多路复用请求以及同时轮询多个连接的能力。

<u>**（1）何时创建 TCP 连接**</u>

Kafka 生产者创建连接有三个时机：

1）在**创建 KafkaProducer 实例时**，生产者应用会在后台创建并启动一个名为 Sender 的线程，该 Sender 线程开始运行时，首先会创建与 `bootstrap.servers` 中所有 Broker 的 TCP 连接。

2）当 Producer **更新集群的元数据信息**之后，如果发现与某些 Broker 当前没有连接，那么它就会创建一个 TCP 连接。

- 场景一：当 Producer 尝试给一个不存在的主题发送消息时，Broker 会告诉 Producer 说这个主题不存在。此时 Producer 会发送 METADATA 请求给 Kafka 集群，去尝试获取最新的元数据信息。
- 场景二：Producer 通过 `metadata.max.age.ms` 参数定期地去更新元数据信息。该参数的默认值是 300000，即 5 分钟，也就是说不管集群那边是否有变化，Producer 每 5 分钟都会强制刷新一次元数据以保证它是最及时的数据。

3）当要**发送消息时**，Producer 发现尚不存在与目标 Broker 的连接，会创建一个 TCP 连接。

**<u>（2）何时关闭 TCP 连接</u>**

Producer 端关闭 TCP 连接的方式有两种：**一种是用户主动关闭；一种是 Kafka 自动关闭**。

- 主动关闭是指调用 `producer.close()` 方法来关闭生产者连接；甚至包括用户调用 `kill -9` 主动“杀掉”Producer 应用。
- 如果设置 Producer 端 `connections.max.idle.ms` 参数大于 0（默认为 9 分钟），意味着，在 `connections.max.idle.ms` 指定时间内，如果没有任何请求“流过”某个 TCP 连接，那么 Kafka 会主动帮你把该 TCP 连接关闭。如果设置该参数为 `-1`，TCP 连接将成为永久长连接。

值得注意的是，在第二种方式中，TCP 连接是在 Broker 端被关闭的，但其实这个 TCP 连接的发起方是客户端，因此在 TCP 看来，这属于被动关闭的场景，即 passive close。被动关闭的后果就是会产生大量的 CLOSE_WAIT 连接，因此 Producer 端或 Client 端没有机会显式地观测到此连接已被中断。

### ==分区：主题（Topic）、分区（Partition）、消息（Record）==

**<u>（1）什么是分区</u>**

​	Kafka 的数据结构采用三级结构，即：主题（Topic）、分区（Partition）、消息（Record）。在 Kafka 中，任意一个 Topic 维护了一组 Partition 日志，如下所示：

![img](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/kafka-log-anatomy.png)

​	每个 Partition 都是一个**单调递增的、不可变的<font color = '#8D0101'>日志记录</font>**，以不断追加的方式写入数据。Partition 中的每条记录会被分配一个**单调递增的 <font color = '#8D0101'>id 号</font>，称为偏移量（Offset）**，用于唯一标识 Partition 内的每条记录。

**<u>（2）为什么要分区</u>**

**为什么 Kafka 的数据结构采用三级结构？**

- **分区的作用就是提供负载均衡的能力**，以实现系统的高伸缩性（Scalability）。
- **不同的分区能够被放置到不同节点的机器上**，而<font color = '#8D0101'>**数据的读写操作也都是针对分区这个粒度而进行的**</font>，这样每个节点的机器都能独立地执行各自分区的读写请求处理。并且，我们还可以通过添加新的机器节点来增加整体系统的吞吐量

**<u>（3）分区策略</u>**

​	所谓分区策略是决定生产者将消息发送到哪个分区的算法，也就是**负载均衡算法**。

​	前文中已经提到，Kafka 生产者发送消息使用的对象 `ProducerRecord` ，可以选填 Partition 和 Key。不过，大多数应用会用到 key。key 有两个作用：

- 作为消息的附加信息；
- 也可以用来决定消息该被写到 Topic 的哪个 Partition，拥有相同 key 的消息将被写入同一个 Partition。

**如果 `ProducerRecord` 指定了 Partition，则分区器什么也不做**，否则分区器会根据 key 选择一个 Partition 。

- 没有 key 时的分发逻辑：每隔 `topic.metadata.refresh.interval.ms` 的时间，随机选择一个 partition。这个时间窗口内的所有记录发送到这个 partition。发送数据出错后会重新选择一个 partition。
- 根据 key 分发：Kafka 的选择分区策略是：根据 key 求 hash 值，然后将 hash 值对 partition 数量求模。这里的关键点在于，**同一个 key 总是被映射到同一个 Partition 上**。所以，在选择分区时，Kafka 会使用 Topic 的所有 Partition ，而不仅仅是可用的 Partition。这意味着，**如果写入数据的 Partition 是不可用的，那么就会出错**。

**<u>（4）自定义分区策略</u>**

如果 Kafka 的默认分区策略无法满足实际需要，可以自定义分区策略。需要显式地配置生产者端的参数 `partitioner.class`。这个参数该怎么设定呢？

- 首先，要实现 `org.apache.kafka.clients.producer.Partitioner` 接口。这个接口定义了两个方法：`partition` 和 `close`，通常只需要实现最重要的 `partition` 方法。我们来看看这个方法的方法签名：

```java
int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster);
```

- 这里的 `topic`、`key`、`keyBytes`、`value`和 `valueBytes` 都属于消息数据，`cluster` 则是集群信息（比如当前 Kafka 集群共有多少主题、多少 Broker 等）。Kafka 给你这么多信息，就是希望让你能够充分地利用这些信息对消息进行分区，计算出它要被发送到哪个分区中。

接着，设置 `partitioner.class` 参数为自定义类的全限定名，那么生产者程序就会按照你的代码逻辑对消息进行分区。

## 两阶段提交

Kafka 的事务机制基于 **两阶段提交（2PC）协议** ，通过协调生产者、消费者与 Kafka 集群之间的操作，确保跨分区和跨会话的原子性。

【**两阶段事务提交**】

**第一阶段：准备提交（Prepare Commit）**
生产者调用 commitTransaction API 后，协调者（Kafka集群中的一个Broker担任协调者，负责管理事务日志（`__transaction_state`主题））将事务状态更新为 “prepare_commit” ，并将该状态写入事务日志（持久化）。
协调者向所有涉及事务的分区（生产者写入的分区和消费者位移提交的分区）发送 事务控制消息 ，标记事务进入准备阶段 。
**第二阶段：正式提交（Commit）**
协调者等待所有分区确认准备完成后，向分区发送 提交指令 ，确保数据对消费者可见 。
若任一分区失败，协调者会触发回滚（abortTransaction），保证原子性 。

【**Flink 和 Kafka 的两阶段提交**】

​	Flink 是一个流处理框架，通过其 checkpoint 机制支持 Exactly-Once 语义。当与 Kafka 集成时，Flink 使用两阶段提交协议来确保数据在源、处理和输出端的一致性。Kafka 从 0.11 版本开始支持事务，这为实现端到端 Exactly-Once 提供了必要支持。

**两阶段提交的过程**

- **预提交阶段**：Flink 发起 checkpoint，sink 算子（如 Kafka 生产者）将数据写入 Kafka 的事务中，但不提交，同时保存状态快照。
- **提交阶段**：checkpoint 成功后，Flink 通知 sink 算子提交事务，Kafka 生产者完成事务提交，使数据对消费者可见

如果 checkpoint 失败，事务会被中止；如果提交前发生故障，恢复时会从 checkpoint 恢复并提交未完成的事务。

> [!WARNING]
>
> **两阶段提交协议的详细实现**
>
> 两阶段提交协议包括预提交（Pre-Commit）和提交（Commit）两个阶段，具体在 Flink 和 Kafka 中的实现如下：
>
> - 预提交阶段
>   - 当 Flink 发起 checkpoint 时，JobMaster（协调者）发送 start-checkpoint 信号。
>   - 各算子执行本地 checkpoint，包括 sink 算子（如 FlinkKafkaProducer）。在此阶段，sink 算子调用 beginTransaction 开始 Kafka 事务，并通过 preCommit 将数据刷新（flush）到 Kafka，但不提交。
>   - 同时，算子的状态（包括事务状态）被快照保存，确保故障恢复时能恢复一致性。
> - 提交阶段
>   - 一旦所有算子完成本地 checkpoint 并返回确认，JobMaster 认为 checkpoint 成功，发送 notifyCheckpointComplete 回调。
>   - sink 算子收到回调后，调用 commit 方法，提交 Kafka 事务，使数据对消费者可见。

## Kafka 消费者

### 消费者简介

**<u>（1）pull 模式</u>**

消息引擎获取消息有两种模式：

- push 模式：MQ 推送数据给消费者
- pull 模式：消费者主动向 MQ 请求数据

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210425190248.png" alt="img" style="zoom:60%;" />

Kafka 消费者（Consumer）以 pull 方式从 Broker 拉取消息。相比于 push 方式，pull 方式灵活度和扩展性更好，因为消费的主动性由消费者自身控制。

push 模式的优缺点：

- 缺点：由 broker 决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。

push 模式的优缺点：

- 优点：consumer可以根据自己的消费能力自主的决定消费策略
- 缺点：如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到达。为了避免这点，Kafka有个参数可以让consumer阻塞直到新消息到达

**<u>（2）消费者</u>**

​	每个 Consumer 的**唯一元数据是该 Consumer 在日志中消费的位置**。这个偏移量是由 Consumer 控制的：Consumer 通常会在读取记录时线性的增加其偏移量。但实际上，由于位置由 Consumer 控制，所以 Consumer 可以采用任何顺序来消费记录。

**一条消息只有被提交，才会被消费者获取到**。如下图，只能消费 Message0、Message1、Message2：

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20200621113917.png" alt="img" style="zoom:67%;" />

**<u>（3）消费者群组</u>**

​	**Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制**。Kafka 的写入数据量很庞大，如果只有一个消费者，消费消息速度很慢，时间长了，就会造成数据积压。为了减少数据积压，Kafka 支持消费者群组，可以让多个消费者并发消费消息，对数据进行分流。

- Kafka 消费者从属于消费者群组，**一个群组里的 Consumer 订阅同一个 Topic，一个主题有多个 Partition，每一个 Partition 只能隶属于消费者群组中的一个 Consumer**。
- 如果超过主题的分区数量，那么有一部分消费者就会被闲置，不会接收到任何消息。
- 同一时刻，**一条消息只能被同一消费者组中的一个消费者实例消费**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210408194235.png" alt="img" style="zoom:35%;" />

- **不同消费者群组之间互不影响**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210408194839.png" alt="img" style="zoom:45%;" />

**<u>（4）消费流程</u>**

Kafka 消费者通过 `poll` 来获取消息，但是获取消息时并不是立刻返回结果，需要考虑两个因素：

- 消费者通过 `customer.poll(time)` 中设置**等待时间**
- Broker 会等待**累计一定量数据，然后发送给消费者**。这样可以减少网络开销。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210425194822.png" alt="img" style="zoom:67%;" />

poll 除了获取消息外，还有其他作用：

- **发送心跳信息**。消费者通过向被指派为群组协调器的 Broker 发送心跳来维护他和群组的从属关系，当机器宕掉后，群组协调器触发再均衡。

### 消费者 API

**<u>（1） 创建消费者</u>**

```java
Properties props = new Properties();
// 服务器地址
props.put("bootstrap.servers", "localhost:9092");
// 消费者群组
props.put("group.id", "test");
// 关闭自动提交偏移量
props.put("enable.auto.commit", "false");
// 设置 key 反序列化器
props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
// 设置 value 反序列化器
props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
```

**<u>（2）订阅主题</u>**

```java
// 订阅主题列表
consumer.subscribe(Arrays.asList("t1", "t2"));
// 订阅所有与 test 相关的主题
consumer.subscribe("test.*");
```

​	`subscribe` 方法允许传入一个正则表达式，这样就可以匹配多个主题。如果有人创建了新的主题，并且主题名恰好匹配正则表达式，那么会立即触发一次分区再均衡，消费者就可以读取新添加的主题。

**<u>（3）轮询获取消息</u>**

​	消息轮询是消费者 API 的核心。一旦消费者订阅了主题，轮询就会处理所有细节，包括：**群组协调、分区再均衡、发送心跳和获取数据**。

```java
try {
    // 3. 轮询
    while (true) {
        // 4. 消费消息
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            log.debug("topic = {}, partition = {}, offset = {}, key = {}, value = {}",
                record.topic(), record.partition(),
                record.offset(), record.key(), record.value());
        }
    }
} finally {
    // 5. 退出程序前，关闭消费者
    consumer.close();
}
```

**<u>（4）手动提交偏移量</u>**

**<u>1）同步提交</u>**

​	**使用 `commitSync()` 提交偏移量最简单也最可靠**。这个 API 会提交由 `poll()` 方法返回的最新偏移量，提交成功后马上返回，如果提交失败就抛出异常。

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(100);
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("topic = %s, partition = %s, offset = %d, customer = %s, country = %s\n",
            record.topic(), record.partition(),
            record.offset(), record.key(), record.value());
    }
    try {
        consumer.commitSync();
    } catch (CommitFailedException e) {
        log.error("commit failed", e)
    }
}
```

同步提交的缺点：**同步提交方式会一直阻塞，直到接收到 Broker 的响应请求，这会大大限制吞吐量**。

**<u>2）异步提交</u>**

​	**在成功提交或碰到无法恢复的错误之前，`commitSync()` 会一直重试，但是 `commitAsync()` 不会**，这也是 `commitAsync()` 不好的一个地方。**它之所以不进行重试，是因为在它收到服务器响应的时候，可能有一个更大的偏移量已经提交成功**。假设我们发出一个请求用于提交偏移量 2000，这个时候发生了短暂的通信问题，服务器收不到请求，自然也不会作出任何响应。与此同时，我们处理了另外一批消息，并成功提交了偏移量 3000。如果 `commitAsync()` 重新尝试提交偏移量 2000，它有可能在偏移量 3000 之后提交成功。这个时候**如果发生再均衡，就会出现重复消息**。

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("topic = %s, partition = %s, offset = % d, customer = %s, country = %s\n ",
            record.topic(), record.partition(), record.offset(),
            record.key(), record.value());
    }
    consumer.commitAsync();
}
```

​	**`commitAsync()` 也支持回调**，在 Broker 作出响应时会执行回调。**回调经常被用于记录提交错误或生成度量指标，不过如果要用它来进行重试，则一定要注意提交的顺序**。

```java
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("topic = %s, partition = %s, offset = % d, customer = %s, country = %s\n ",
            record.topic(), record.partition(), record.offset(),
            record.key(), record.value());
    }
    consumer.commitAsync(new OffsetCommitCallback() {
        @Override
        public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception e) {
            if (e != null) { log.error("Commit failed for offsets {}", offsets, e); }
        }
    });
}
```

> **重试异步提交**
>
> 可以使用一个单调递增的序列号来维护异步提交的顺序。在每次提交偏移量之后或在回调里提交偏移量时递增序列号。在进行重试前，先检查回调的序列号和即将提交的偏移量是否相等，如果相等，说明没有新的提交，那么可以安全地进行重试；如果序列号比较大，说明有一个新的提交已经发送出去了，应该停止重试。

**<u>3）同步和异步组合提交</u>**

​	一般情况下，针对偶尔出现的提交失败，不进行重试不会有太大问题，因为如果提交失败是因为临时问题导致的，那么后续的提交总会有成功的。但**如果这是发生在关闭消费者或再均衡前的最后一次提交，就要确保能够提交成功**。

​	因此，在消费者关闭前一般会组合使用 `commitSync()` 和 `commitAsync()`。

```java
try {
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
        for (ConsumerRecord<String, String> record : records) {
            System.out.printf("topic = %s, partition = %s, offset = % d, customer = %s, country = %s\n ",
                record.topic(), record.partition(), record.offset(), record.key(), record.value());
        }
        consumer.commitAsync();
    }
} catch (Exception e) {
    log.error("Unexpected error", e);
} finally {
    try {
        consumer.commitSync();
    } finally {
        consumer.close();
    }
}
```

**<u>4）提交特定的偏移量</u>**

​	提交偏移量的频率和处理消息批次的频率是一样的。如果想要更频繁地提交该怎么办？如果 `poll()` 方法返回一大批数据，为了避免因再均衡引起的重复处理整批消息，想要在批次中间提交偏移量该怎么办？这种情况无法通过调用 `commitSync()` 或 `commitAsync()` 来实现，因为它们只会提交最后一个偏移量，而此时该批次里的消息还没有处理完。

​	解决办法是：**消费者 API 允许在调用 `commitSync()` 和 `commitAsync()` 方法时传进去希望提交的分区和偏移量的 map**。

```java
private int count = 0;
private final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new HashMap<>();

while (true) {
  ConsumerRecords<String, String> records = consumer.poll(100);
  for (ConsumerRecord<String, String> record : records) {
    System.out.printf("topic = %s, partition = %s, offset = % d, customer = %s, country = %s\n ",
                      record.topic(), record.partition(), record.offset(), record.key(), record.value());

    currentOffsets.put(new TopicPartition(record.topic(),
                                          record.partition()), new
                       OffsetAndMetadata(record.offset() + 1, "no metadata"));
    if (count % 1000 == 0) { consumer.commitAsync(currentOffsets, null); }
    count++;
  }
}
```

**<u>5）从特定偏移量处开始处理</u>**

​	**如果想让消费者从轮询消费消息的无限循环中退出，可以通过另一个线程调用 `consumer.wakeup()` 方法**。 **`consumer.wakeup()` 是消费者唯一一个可以从其他线程里安全调用的方法**。调用 `consumer.wakeup()` 可以退出 `poll()` ，并抛出 `WakeupException` 异常，或者如果调用 `consumer.wakeup()` 时线程没有等待轮询，那么异常将在下一轮调用 `poll()` 时抛出。

```java
Runtime.getRuntime().addShutdownHook(new Thread() {
    public void run() {
        System.out.println("Starting exit...");
        consumer.wakeup();
        try {
            mainThread.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
});

...

try {
    // looping until ctrl-c, the shutdown hook will cleanup on exit
    while (true) {
        ConsumerRecords<String, String> records =
            movingAvg.consumer.poll(1000);
        System.out.println(System.currentTimeMillis() +
            "--  waiting for data...");
        for (ConsumerRecord<String, String> record : records) {
            System.out.printf("offset = %d, key = %s, value = %s\n",
                record.offset(), record.key(), record.value());
        }
        for (TopicPartition tp: consumer.assignment())
            System.out.println("Committing offset at position:" +
                consumer.position(tp));
            movingAvg.consumer.commitSync();
    }
} catch (WakeupException e) {
    // ignore for shutdown
} finally {
    consumer.close();
    System.out.println("Closed consumer and we are done");
}
```

### 提交偏移量

​	每次调用 `poll()` 方法，它总是会返回由生产者写入 Kafka 但还没有被消费者读取过的记录，Kafka 因此可以追踪哪些记录是被哪个群组的哪个消费者读取的。

<font color = '#8D0101'>**更新分区当前位置的操作叫作提交**</font>

**<u>（1）偏移量的用处</u>**

​	如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果**消费者发生崩溃或有新的消费者加入群组，就会触发再均衡**，完成再均衡后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。

**==如何从 Partition 中通过 Offset 查找 Message？==**

- 基于以下的数据和图形，如何读取 offset=170425 的 Message（消息）呢？关键在于通过 Offset 定位出消息的物理偏移地址。首先，列出各个 Segment 的索引文件及其偏移量范围，如下：

  ```css
  索引文件1：00000000000000000000.index，偏移量范围：[0，170410]；
  索引文件2：00000000000000170410.index，偏移量范围：[170410+1，239430]；
  索引文件3：00000000000000239430.index，偏移量范围：[239430+1，239430+1+X]，X大于1，具体值与消息量和大小有关；
  ```

- 根据索引文件的偏移量范围，`170410< offset=170425 <239430`，因此，Offset=170425 对应的索引文件为 `00000000000000170410.index`。索引文件是“有序的”，通过二分查找（又称折半查找）便可快速定位具体文件位置。此后，根据 `00000000000000170410.index` 文件中的 `[15,2369]` 定位到数据文件 `00000000000000170410.log` 中的 2369 位置，这便是目标消息的位置，读取即可。

**1）如果提交的偏移量小于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息就会被重复处理**

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210412200354.png" alt="img" style="zoom:67%;" />

**2）如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。**

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210412200405.png" alt="img" style="zoom:75%;" />

**<u>（2）提交偏移量的旧方案</u>**

​	**老版本的 Consumer Group 把偏移量保存在 ZooKeeper 中**。ZooKeeper 是一个分布式的协调服务框架，Kafka 重度依赖它实现各种各样的协调管理。将偏移量保存在 ZooKeeper 外部系统的做法，最显而易见的好处就是减少了 Kafka Broker 端的状态保存开销，有利于实现伸缩性。

​	这种方案的问题在于：ZooKeeper 其实并不适合进行高频的写操作，而 Consumer Group 的偏移量更新却是一个非常频繁的操作。这种大吞吐量的写操作会极大地拖慢 ZooKeeper 集群的性能，因此 Kafka 社区渐渐有了这样的共识：将 Consumer 偏移量保存在 ZooKeeper 中是不合适的做法。

**<u>（3）提交偏移量的新方案</u>**

新版本 Consumer 的偏移量管理机制其实也很简单。

​	消费者向一个叫做<font color = '#8D0101'> **`_consumer_offsets` 的特殊主题**</font>发送消息，**消息里包含每个分区的偏移量**。如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果消费者发生崩溃或有新的消费者加入群组，就会**触发再均衡**，完成再均衡后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。

- **`_consumer_offsets` 主题的 Key 中应该保存 3 部分内容：`<Group ID，主题名，分区号 >`**。

​	通常来说，**当 Kafka 集群中的第一个 Consumer 程序启动时，Kafka 会自动创建偏移量主题**。偏移量主题就是普通的 Kafka 主题，那么它自然也有对应的分区数。**如果偏移量主题是 Kafka 自动创建的，那么该主题的分区数是 50，副本数是 3**。分区数可以通过 `offsets.topic.num.partitions` 设置；副本数可以通过 `offsets.topic.replication.factor` 设置。

**（4）手动提交**

​	**自动提交虽然方便，不过无法避免丢失消息和分区再均衡时重复消息的问题**。因此，可以通过手动提交偏移量，由开发者自行控制。

- 首先，**把 `enable.auto.commit` 设为 false，关闭自动提交**。
- 如果 Kafka 触发了再均衡，我们需要在消费者失去对一个分区的所有权之前提交最后一个已处理记录的偏移量。如果消费者准备了一个缓冲区用于处理偶发的事件，那么在失去分区所有权之前，需要处理在缓冲区累积下来的记录。可能还需要关闭文件句柄、数据库连接等。

在为消费者分配新分区或移除旧分区时，可以通过消费者 API 执行一些应用程序代码，在调用 `subscribe()` 方法时传进去一个 `ConsumerRebalanceListener` 实例就可以了。 `ConsumerRebalanceListener` 有两个需要实现的方法。

- `public void onPartitionsRevoked(Collection partitions)` 方法会在再均衡开始之前和消费者停止读取消息之后被调用。如果在这里提交偏移量，下一个接管分区的消费者就知道该从哪里开始读取了。
- `public void onPartitionsAssigned(Collection partitions)` 方法会在重新分配分区之后和消费者开始读取消息之前被调用。

### 分区再均衡

**<u>（1）什么是分区再均衡</u>**

- 分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为**分区再均衡（Rebalance）**。**Rebalance 实现了消费者群组的高可用性和伸缩性**。
- **Rebalance 本质上是一种协议，规定了一个 Consumer Group 下的所有 Consumer 如何达成一致，来分配订阅 Topic 的每个分区**。比如某个 Group 下有 20 个 Consumer 实例，它订阅了一个具有 100 个分区的 Topic。正常情况下，Kafka 平均会为每个 Consumer 分配 5 个分区。这个分配的过程就叫 Rebalance。
- 当在群组里面 新增/移除消费者 或者 新增/移除 kafka 集群 broker 节点 时，群组协调器 Broker 会触发再均衡，重新为每一个 Partition 分配消费者。**Rebalance 期间，消费者无法读取消息，造成整个消费者群组一小段时间的不可用。**

**<u>（2）何时触发分区再均衡</u>**

分区再均衡的触发时机有三种：

- **消费者群组成员数发生变更**

  。比如有新的 Consumer 加入群组或者离开群组，或者是有 Consumer 实例崩溃被“踢出”群组。

  - 新增消费者。consumer 订阅主题之后，第一次执行 poll 方法
  - 移除消费者。**<font color = '#8D0101'>执行 `consumer.close()` 操作或者消费客户端宕机</font>**，就不再通过 poll 向群组协调器发送心跳了，当群组协调器**检测次消费者没有心跳，就会触发再均衡**。

- **订阅主题数发生变更**。Consumer Group 可以使用正则表达式的方式订阅主题，比如 `consumer.subscribe(Pattern.compile(“t.*c”))` 就表明该 Group 订阅所有以字母 t 开头、字母 c 结尾的主题。在 Consumer Group 的运行过程中，你新创建了一个满足这样条件的主题，那么该 Group 就会发生 Rebalance。

- **订阅主题的分区数发生变更**

  。Kafka 当前只能允许增加一个主题的分区数。当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。

  - 新增 broker。如重启 broker 节点
  - 移除 broker。如 kill 掉 broker 节点。

**<u>（3）分区再均衡的过程</u>**

**Rebalance 是通过消费者群组中的称为<font color = '#8D0101'>“群主”消费者客户端</font>进行的**。

**<u>1）选择群主</u>**

当消费者要加入群组时，会向群组协调器发送一个 JoinGroup 请求。第一个加入群组的消费者将成为“群主”。**群主从协调器（一个Broker）那里获取群组的活跃成员列表，并负责给每一个消费者分配分区**。

> 所谓协调者，在 Kafka 中对应的术语是 Coordinator，它专门为 Consumer Group 服务，负责为 Group 执行 Rebalance 以及提供位移管理和组成员管理等。具体来讲，Consumer 端应用程序在提交位移时，其实是向 Coordinator 所在的 Broker 提交位移。同样地，当 Consumer 应用启动时，也是向 Coordinator 所在的 Broker 发送各种请求，然后由 Coordinator 负责执行消费者组的注册、成员管理记录等元数据管理操作。

2）消费者通过向被指派为群组协调器（Coordinator）的 Broker 定期发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210415160730.png" alt="img" style="zoom:60%;" />

3）群主从群组协调器获取群组成员列表，然后给每一个消费者进行分配分区 Partition。有两种分配策略：Range 和 RoundRobin。

- **Range 策略**，就是把若干个连续的分区分配给消费者，如存在分区 1-5，假设有 3 个消费者，则消费者 1 负责分区 1-2,消费者 2 负责分区 3-4，消费者 3 负责分区 5。
- **RoundRoin 策略**，就是把所有分区逐个分给消费者，如存在分区 1-5，假设有 3 个消费者，则分区 1->消费 1，分区 2->消费者 2，分区 3>消费者 3，分区 4>消费者 1，分区 5->消费者 2。

4）群主分配完成之后，把分配情况发送给群组协调器。

5）群组协调器再把这些信息发送给消费者。**每个消费者只能看到自己的分配信息，只有群主知道所有消费者的分配信息**。

## Kafka集群

### 控制器（Controller）

​	控制器（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 ZooKeeper 的帮助下管理和协调整个 Kafka 集群。**控制器其实就是一个 Broker**，只不过它除了具有一般 Broker 的功能以外，还负责 **Leader 的选举**。

> [!CAUTION]
>
> **<font color = '#8D0101'>这里的Leader指的是分区的Leader</font>**，如下图，比如分区1的Leader是位于BROKER2的，意思是BROKER2为分区1的Leader，分区2的Leader处于BROKER1，即BROKER1为分区2的Leader

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/image-20250304160607139.png" alt="image-20250304160607139" style="zoom:35%;" />

**<u>（1）控制器选举流程</u>**

​	集群中任意一台 Broker 都能充当控制器的角色，但是，在运行过程中，只能有一个 Broker 成为控制器，行使其管理和协调的职责。实际上，Broker 在启动时，会尝试去 ZooKeeper 中创建 `/controller` 节点。Kafka 当前选举控制器的规则是：**第一个在 ZooKeeper 成功创建 `/controller` 临时节点的 Broker 会被指定为控制器**。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210502213820.png" alt="img" style="zoom:75%;" />

**详细流程：**

1. 第一个在 ZooKeeper 中成功创建 **`/controller` 临时节点的 Broker 会被指定为控制器**。
2. 其他 Broker 在控制器节点上**创建 Zookeeper watch 对象**。
3. 如果控制器被关闭或者与 Zookeeper 断开连接，Zookeeper 临时节点就会消失。集群中的其他 Broker 通过 watch 对象得到状态变化的通知，它们会**尝试让自己成为新的控制器**。
4. 第一个在 Zookeeper 里创建一个临时节点 `/controller` 的 Broker 成为新控制器。其他 Broker **<font color = '#8D0101'>在新控制器节点上</font>创建<font color = '#8D0101'> Zookeeper watch 对象</font>**。
5. **每个新选出的控制器通过 Zookeeper 的条件递增操作获得一个全新的、数值更大的 controller epoch。其他节点会忽略旧的 epoch 的消息。**
6. 当控制器发现一个 Broker 已离开集群，并且这个 Broker 是某些 Partition 的 Leader。此时，控制器会遍历这些 Partition，并用轮询方式确定谁应该成为新 Leader，随后，新 Leader 开始处理生产者和消费者的请求，而 Follower 开始从 Leader 那里复制消息。

​	简而言之，**Kafka 使用 Zookeeper 的临时节点来选举控制器，并在节点加入集群或退出集群时通知控制器。控制器负责在节点加入或离开集群时进行 Partition Leader 选举。控制器使用 epoch 来避免“脑裂”，“脑裂”是指两个节点同时被认为自己是当前的控制器**。

**<u>（2）控制器的作用</u>**

**<u>1） Topic 管理（创建、删除、增加分区）</u>**

这里的 Topic 管理，就是指控制器帮助我们完成对 Kafka Topic 的创建、删除以及分区增加的操作。换句话说，当我们执行 **kafka-topics 脚本**时，大部分的后台工作都是控制器来完成的。

**<u>2）选举 Leader</u>**

Preferred 领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。

**<u>3）集群成员管理</u>**

集群成员管理，包括自动检测新增 Broker、Broker 主动关闭及被动宕机。这种自动检测是依赖于前面提到的 **Watch 功能和 ZooKeeper 临时节点组合实现的**。

- 比如，控制器组件会利用**Watch 机制**检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。
- 侦测 Broker 存活性则是依赖于刚刚提到的另一个机制：**临时节点**。每个 Broker 启动后，会在 /brokers/ids 下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行“善后”。

**<u>4）数据服务</u>**

控制器的最后一大类工作，就是向其他 Broker 提供数据服务。控制器上保存了最全的集群元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。

控制器中保存了多种数据，比较重要的的数据有：

- 所有 Topic 信息。包括具体的分区信息，比如领导者副本是谁，ISR 集合中有哪些副本等。
- 所有 Broker 信息。包括当前都有哪些运行中的 Broker，哪些正在关闭中的 Broker 等。
- 所有涉及运维任务的分区。包括当前正在进行 Preferred 领导者选举以及分区重分配的分区列表。

值得注意的是，这些数据其实在 ZooKeeper 中也保存了一份。每当控制器初始化时，它都会从 ZooKeeper 上读取对应的元数据并填充到自己的缓存中。有了这些数据，控制器就能对外提供数据服务了。这里的对外主要是指对其他 Broker 而言，控制器通过向这些 Broker 发送请求的方式将这些数据同步到其他 Broker 上。

### 副本机制（Leader和Follower，ISR）

副本机制是分布式系统实现高可用的不二法门，Kafka 也不例外。

副本机制有哪些好处？

1. **提供可用性**：有句俗语叫：鸡蛋不要放在一个篮子里。副本机制也是一个道理——当部分节点宕机时，系统仍然可以依靠其他正常运转的节点，从整体上对外继续提供服务。
2. **提供伸缩性**：通过增加、减少机器可以控制系统整体的吞吐量。
3. **改善数据局部性**：允许将数据放入与用户地理位置相近的地方，从而降低系统延时。

但是，Kafka 只实现了第一个好处，原因后面会阐述。

<u>**（1）Kafka副本角色：Leader和Follower**</u>

​	Kafka 使用 Topic 来组织数据，每个 Topic 被分为若干个 Partition，每个 Partition 有多个副本。每个 Broker 可以保存成百上千个属于不同 Topic 和 Partition 的副本。<font color = '#8D0101'>**Kafka 副本的本质是一个只能追加写入的提交日志**</font>。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210407180101-20250304162436774.png" alt="img" style="zoom:50%;" />

**Kafka 副本有两种角色**：

- **Leader 副本（主）**：每个 Partition 都有且仅有一个 Leader 副本。为了保证数据一致性，**Leader 处理一切对 Partition （分区）的读写请求**；
- **Follower 副本（从）**：Leader 副本以外的副本都是 Follower 副本。**Follower 唯一的任务就是从 Leader 那里复制消息，保持与 Leader 一致的状态**。
- 如果 Leader 宕机，其中一个 Follower 会被选举为新的 Leader。

<img src="https://raw.githubusercontent.com/GLeXios/Notes/main/pics/20210407191337.png" alt="img" style="zoom:50%;" />

​	为了与 Leader 保持同步，**Follower 向 Leader 发起获取数据的请求pull，这种请求与消费者为了读取消息而发送的请求是一样的**。请求消息里包含了 Follower 想要获取消息的偏移量，而这些偏移量总是有序的。

​	Leader 另一个任务是搞清楚哪个 Follower 的状态与自己是一致的。通过查看每个 Follower 请求的最新偏移量，Leader 就会知道每个 Follower 复制的进度。如果跟随者在 10s 内没有请求任何消息，或者虽然在请求消息，但是在 10s 内没有请求最新的数据，那么它就会被认为是**不同步**的。**如果一个副本是不同步的，在 Leader 失效时，就不可能成为新的 Leader**——毕竟它没有包含全部的消息。

​	除了当前首领之外，每个分区都有一个首选首领——创建 Topic 时选定的首领就是分区的首选首领。之所以叫首选 Leader，是因为在创建分区时，需要在 Broker 之间均衡 Leader。

**<u>（2）ISR</u>**

​	ISR 即 In-sync Replicas，表示同步副本。Follower 副本不提供服务，只是定期地**<font color = '#8D0101'>异步拉取</font>**领导者副本中的数据而已。既然是异步的，说明和 Leader 并非数据强一致性的。

**判断 Follower 是否与 Leader 同步的标准**：

​	Kafka Broker 端参数 `replica.lag.time.max.ms` 参数，指定了 Follower 副本能够落后 Leader 副本的最长时间间隔，默认为 10s。这意味着：只要一个 Follower 副本落后 Leader 副本的时间不连续超过 10 秒，那么 Kafka 就认为该 Follower 副本与 Leader 是**同步**的，即使此时 Follower 副本中保存的消息明显少于 Leader 副本中的消息。

> [!IMPORTANT]
>
> ISR 是一个**<font color = '#8D0101'>动态调整的集合（所有同步的副本都在ISR中，同时排除掉不同步的副本）</font>**，会不断将同步副本加入集合，将不同步副本移除集合。Leader 副本天然就在 ISR 中。

###  Kafka集群总结

**<u>（1）副本机制</u>**

- 每个 Partition 都有一个 Leader，零个或多个 Follower。
- Leader 处理一切对 Partition （分区）的读写请求；而 Follower 只需被动的同步 Leader 上的数据。
- 同一个 Topic 的不同 Partition 会分布在多个 Broker 上，而且一个 Partition 还会在其他的 Broker 上面进行备份。

**<u>（2）选举机制</u>**

Follower 宕机，啥事儿没有；Leader 宕机了，会从 Follower 中重新选举一个新的 Leader。 生产者/消费者如何知道谁是 Leader

- Kafka 将这种元数据存储在 Zookeeper 服务中。
- 生产者和消费者都和 Zookeeper 连接并通信。

## 全程解析（Producer-kafka-Consumer）

### 4.1 Producer 发布消息

​	Producer 采用 Push 模式将消息发布到 Kafka Broker，根据负载均衡算法（如轮询、Hash 等），这些消息将均衡写入到相应 Topic 对应的各个 Partition 中。在存储层面，采用顺序写磁盘（即 Append）模式写入。详细流程如下：

1. Producer Push 消息，基于负载均衡算法获得目标 Partition 后，Producer 先从 ZooKeeper 的 `/brokers/.../state` 节点找到该 Partition 的 Leader；
2. Producer 将消息发送给该 Leader；
3. Leader 将消息写入本地 Log；
4. 所有 Follower 主动从 Leader Pull 消息，写入本地 Log 后向 Leader 发送 ACK；
5. Leader 收到所有 ISR 中所有 Replica 的 ACK 后，更新 HW（High Watermark，最后 Commit 的 Offset），并向 Producer 发送 ACK；
6. Producer 接到 ACK，确认发送成功。

### 4.2 Broker 存储消息

​	Topic 是逻辑概念，而 Topic 对应的 Partition 则是物理概念，每个 Partition 在存储层面都对应一个文件夹（目录）。由于 Partition 并不是最终的存储粒度，该文件夹下还有多个 Segment（消息索引和数据文件，它们是真正的存储文件）。

### 4.3 Consumer 消费消息

​	前面介绍过，目前采用的高级 API，Consumer 在消费消息时，只需指定 Topic 即可，API 内部实现负载均衡，并将 Offset 记录到 ZooKeeper 上。

​	值得一提的是，Consumer 采用 Pull 模式从 Broker 中读取数据，这是一种异步消费模式，与 Producer 采用的 Push 模式全然不同。Push 模式追求速度，越快越好，当然它取决于 Broker 的性能，而 Pull 模式则是追求自适应能力，Consumer 根据自己的消费能力消费。

# Yarn

## hadoop yarn 简介

​	**Apache YARN** (Yet Another Resource Negotiator) 是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。

![img](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f7961726e2d626173652e706e67.png)

## YARN架构

### ResourceManager

​	**`ResourceManager` 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者**。`ResourceManager` 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。它主要由两个组件 构成：调度器（Scheduler）和应用程序管理器（Applications Manager，AM）。

- **调度器：**调度器根据容量、队列等限制条件（如每个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。
  - 该调度器是 一个“纯调度器”，它不再从事任何与具体应用程序相关的工作，比如不负责监控或者跟踪 应用的执行状态等，也不负责重新启动因应用执行失败或者硬件故障而产生的失败任务， 这些均交由应用程序相关的 ApplicationMaster 完成。**调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念“资源容器”（Resource Container，简称 Container）表示**，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。
- **应用程序管理器（Applications Manager，AM）**
  - 应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动 ApplicationMaster、监控 ApplicationMaster 运行状态并在失败时重新启动它等。

### NodeManager

​	**`NodeManager` 是 YARN 集群中的每个具体节点的管理者**。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下：

- 启动时向 `ResourceManager` 注册并定时发送心跳消息，等待 `ResourceManager` 的指令；
- 维护 `Container` 的生命周期，监控 `Container` 的资源使用情况；
- 管理任务运行时的相关依赖，根据 `ApplicationMaster` 的需要，在启动 `Container` 之前将需要的程序及其依赖拷贝到本地。

### ApplicationMaster

​	在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 `ApplicationMaster`。`ApplicationMaster` 负责协调来自 `ResourceManager` 的资源，并通过 `NodeManager` 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下：

- 与 RM 调度器协商以获取资源（用 Container 表示）；
- 将得到的任务进一步分配给内部的任务；
- 与 NM 通信以启动 / 停止任务；
- 监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。

### Container

​	`Container` 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 `Container` 表示的。**YARN 会为每个任务分配一个 `Container`，该任务只能使用该 `Container` 中描述的资源**。`ApplicationMaster` 可在 `Container` 内运行任何类型的任务。例如，`MapReduce ApplicationMaster` 请求一个容器来启动 map 或 reduce 任务，而 `Giraph ApplicationMaster` 请求一个容器来运行 Giraph 任务。

**实例**

​	可将 YARN 看做一个云操作系统，它负责为应用程序启 动 ApplicationMaster（相当于主线程），然后再由 ApplicationMaster 负责数据切分、任务分配、 启动和监控等工作，而由 ApplicationMaster 启动的各个 Task（相当于子线程）仅负责自己的计 算任务。当所有任务计算完成后，ApplicationMaster 认为应用程序运行完成，然后退出

![在这里插入图片描述](https://raw.githubusercontent.com/GLeXios/Notes/main/pics/watermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODg5NzM2NA%3D%3D%2Csize_16%2Ccolor_FFFFFF%2Ct_70.png)

# 

